<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[感知机]]></title>
    <url>%2F2019%2F09%2F17%2F%E6%84%9F%E7%9F%A5%E6%9C%BA%2F</url>
    <content type="text"><![CDATA[感知机（perceptron）是二类分类的线性分类模型，其输入为实例的特征向量，输出为实例的类别，取$+1$和$-1$二值。感知机对应于输入空间（特征空间）中将实例划分为正负两类的分离超平面，属于判别模型。 感知机模型定义假设输入空间（特征空间）是$X\in{R^n}$，输出空间是$Y={+1,-1}$。输入$x\in{X}$表示实例的特征向量，对应于输入空间（特征空间）的点；输出$y\in{Y}$表示实例的类别。由输入空间到输出空间的如下函数 $$f(x)=sign(w\cdot{x}+b)$$ 称为感知机。其中，$w$和$b$为感知机模型的参数，$w\in{R^n}$叫作权值（weight）或权值向量（weight vector），$b\in{R}$叫作偏置（bias），$w\cdot{x}$表示$w$和$x$的内积。$sign$是符号函数，即 $$sign(x)= \begin{cases}+1, &amp; x \geq 0 \-1, &amp; x &lt; 0\end{cases}$$ 感知机是一种线性分类模型，属于判别模型。感知机模型的空间假设是定义在特征空间中的所有线性分类模型（linear classification model）或线性分类器（linear classifier），即函数集合$f|f(x)=w\cdot{x}+b$。 感知机其实就是寻找一个超平面$w\cdot{x}+b=0$将特征空间划分成为两个部分。位于超平面两侧的点分别被分为正负两类。 感知机学习策略数据集的线性可分性定义给定一个数据集 $$T={(x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N)},$$ 其中，$x_i\in{X}=R^n, y_i\in{Y}={+1,-1}, i=1,2,\cdots,N$，如果存在某个超平面能够将数据集的正实例点和负实例点完全正确地划分到超平面的两侧，即对所有$y_i=+1$的实例$i$，有$w\cdot{x}+b&gt;0$；对所有$y_i=-1$的实例$i$，有$w\cdot{x}+b&lt;0$，则称数据集$T$是线性可分数据集(linear separable data set)；否则称数据集$T$线性不可分。 学习策略损失函数是误分类点到超平面$S$的总距离，首先写出输入空间$R^n$中任一点$x_0$到超平面$S$的距离： $$\frac{1}{||w||}|w\cdot{x_0}+b|$$ 其中，$||w||$是$w$的$L_2$范数。其次，对于误分类的数据$(x_i,y_i)$来说，有$-y_i(w\cdot{x}+b)&gt;0$成立。因此误分类点$x_i$到超平面$S$的距离也可以表示成： $$-\frac{1}{||w||}y_i(w\cdot{x_i}+b)$$ 假设超平面$S$对应的误分类点集合为$M$，那么所有误分类点到超平面$S$的总距离为 $$-\frac{1}{||w||}\sum_{x_i\in{M}}y_i(w\cdot{x_i}+b)$$ 不考虑$\frac{1}{||w||}$就得到了感知机学习的损失函数。 给定训练数据集 $$T={(x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N)},$$其中，$x_i\in{X}=R^n, y_i\in{Y}={+1,-1}, i=1,2,\cdots,N$。感知机$sign(w\cdot{x}+b)$学习的损失函数定义为 $$L(w,b)=-\sum_{x_i\in{M}}y_i(w\cdot{x_i}+b)$$其中，$M$是误分类点的集合。 感知机学习算法感知机学习算法有原始形式和对偶形式两种。 原始形式感知机学习算法是对以下最优化问题的算法。给定一个训练数据集 $$T={(x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N)},$$其中，$x_i\in{X}=R^n, y_i\in{Y}={+1,-1}, i=1,2,\cdots,N$，求解参数$w,b$，使其为以下损失函数最小化问题的解 $$min_{w,b}L(w,b)=min_{w,b}-\sum_{x_i\in{M}}y_i(w\cdot{x_i}+b)$$ 其中，$M$是误分类点的集合。 感知机学习算法是误分类驱动的，具体采用随机梯度下降法（stochastic gradient descent）。 感知机学习算法的原始形式 输入：训练数据集$T={(x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N)}$，其中，$x_i\in{X}=R^n, y_i\in{Y}={+1,-1}, i=1,2,\cdots,N$；学习率$\eta(0&lt;\eta\le{1})$；输出：$w,b$；感知机模型$f(x)=sign(w\cdot{x}+b)$。（1） 选取初值$w_0,b_0$；（2） 在训练集中选取数据$(x_i,y_i)$；（3） 如果$y_i(w\cdot{x}+b)\le{0}$，即$x_i$是误分类的点，那么 $$w\leftarrow{w+\eta{y_ix_i}}\b\leftarrow{b+\eta{y_i}}$$ （4） 转至(2)，直至训练集中没有误分类点 算法收敛性证明对于线性可分的数据集感知机学习算法原始形式收敛，即经过有限次迭代可以得到一个将训练数据集完全正确划分的分离超平面及感知机模型。 定理 设训练数据集$T={(x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N)}$是线性可分的，其中存在$x_i\in{X}=R^n, y_i\in{Y}={+1,-1}, i=1,2,\cdots,N$，则（1）存在满足条件$||\hat{w}{opt}||=1$的超平面$\hat{w}{opt}\cdot{\hat{x}}=\hat{w}{opt}\cdot{x}+b{opt}$将训练数据集完全正确分开；且存在$\gamma&gt;0$，对所有$i=1,2,\cdots,N$ $$y_i(\hat{w}_{opt}\cdot{\hat{x_i}})=y_i(\hat{w}{opt}\cdot{x}+b{opt})\geq\gamma$$ （2）令$R=max_{1\le{i}\le{N}}$，则感知机算法的原始形式在训练集上的误分类次数$k$满足不等式 $$k\le(\frac{R}{\gamma})^2$$ 即表示训练次数有上届，能够在有限次训练中找到将数据集完全正确划分的超平面。详细证明可以阅读《统计学习方法》。 感知机学习算法的对偶形式对偶形式的基本思想是将$w$和$b$表示成实例$x_i$和标记$y_i$的线性组合的形式，通过求解其系数而求得$w$和$b$。对于误分类点，更新$w$和$b$如下： $$w\leftarrow{w+\eta{y_ix_i}}\b\leftarrow{b+\eta{y_i}}$$ 逐步更新$w,b$，假设更新$n$次，则$w,b$关于$(x_i,y_i)$的增量分别是$\alpha_iy_ix_i$和$\alpha{y_i}$，这个$\alpha_i=n_i\eta$。这样从学习过程不难看出，最后学习到的$w,b$可以分别表示为 $$w=\sum_{i=1}^N{\alpha_iy_ix_i}\b=\sum_{i=1}^N{\alpha_iy_i}$$ 这里，$a_i\ge0,i=1,2,\cdots,N$，当$\eta=1$时，表示第$i$个实例点由于误分而进行更新的次数。实例点更新次数越多，意味着它距离分离超平面越近，也越难正确分类。 感知机学习算法的对偶形式 输入：训练数据集$T={(x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N)}$，其中，$x_i\in{X}=R^n, y_i\in{Y}={+1,-1}, i=1,2,\cdots,N$；学习率$\eta(0&lt;\eta\le{1})$；输出：$\alpha,b$；感知机模型$f(x)=sign({\sum_{j=1}^N{\alpha_jy_jx_j}}\cdot{x}+b)$。其中，$\alpha=(\alpha_1,\alpha_2,\cdots,\alpha_N)^T$。（1）$\alpha\leftarrow0,b\leftarrow0$（2）在训练集中选取数据$(x_i,y_i)$（3）如果$y_i({\sum_{j=1}^N{\alpha_jy_jx_j}}\cdot{x}+b)\le0$ $$\alpha_i\leftarrow\alpha_i+\eta \b\leftarrow{b+\eta{y_i}}$$ （4）转至（2）直到没有误分类数据。]]></content>
      <categories>
        <category>统计学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[统计学习三要素]]></title>
    <url>%2F2019%2F09%2F10%2F%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E4%B8%89%E8%A6%81%E7%B4%A0%2F</url>
    <content type="text"><![CDATA[统计学习方法是由模型、策略和算法构成的，即统计学习方法由三要素构成，可以简单表示为： $$方法=模型+策略+算法$$ 模型统计学习首先要考虑的问题是学习什么样的模型。在监督学习过程中，模型就是所要学习的条件概率分布$P(Y/X)$或者决策函数$Y=f(X)$。模型的假设空间包含所有可能的条件概率分布或决策函数。 对于决策函数是输入变量的线性函数的情况，模型的假设空间就是所有线性函数构成的函数集合。假设空间用$F$表示。假设空间可以定义为决策函数的集合 $$F={f|Y=f(X)}$$ 其中，$X$和$Y$是定义在输入空间$\LARGE{x}$和输出空间$\LARGE{y}$上的变量。这时$F$通常是由一个参数向量决定的函数族： $$F={f|Y=f_\theta(X),\theta\in{R^n}}$$参数向量$\theta$取值于$n$维欧氏空间$R^n$，称为参数空间。 假设空间也可以定义为条件概率的集合 $$F={P|P(Y/X)}$$其中，$X$和$Y$是定义在输入空间$\LARGE{x}$和输出空间$\LARGE{y}$上的随机变量。这时$F$通常是由一个参数向量决定的条件概率分布族： $$F={P|P_\theta(Y/X)}$$ 参数向量$\theta$取值于$n$维欧氏空间$R^n$，也称为参数空间。 策略有了模型的假设空间，统计学习接着需要考虑使用什么样的策略从假设空间中选取出最优的模型。 首先引入损失函数与风险函数的概念，损失函数模型一次预测的好坏，风险函数度量平均意义下模型预测的好坏。 损失函数监督学习问题是在假设空间$F$中选取模型$f$作为决策函数，对于给定的输入$X$，由$f(X)$给出相应的输出$Y$，这个输出的预测值$f(X)$与真实值$Y$可能一致也可能不一致，用一个损失函数(loss function)与代价函数(cost function)来度量错误的程度。损失函数是$f(X)$与$Y$的非负实值函数，记作$L(Y,f(X))$。 统计学习常用的损失函数有以下几种：（1）0-1损失函数(0-1 loss function) $$L(Y,f(X)) = \begin{cases}1, &amp; Y \neq f(X) \0, &amp; Y = f(X)\end{cases}$$ （2）平方损失函数(quadratic loss function) $$L(Y,f(X)) = (Y-f(X))^2$$ （3）绝对损失函数(absolute loss function) $$L(Y,f(X)) = |Y-f(X)|$$ （4）对数损失函数(logarithmic loss function)或者对数似然损失函数(log-likelihood loss function) $$L(Y,P(Y/X)) = -log{P(Y/X)}$$ 风险函数损失函数值越小，模型就越好。由于模型的输入、输出$(X,Y)$是随机变量，遵循联合分布$P(X,Y)$，所以损失函数的期望是 $$R_{exp}(f)=E_p[L(Y,f(X))]=\int_{\LARGE{x}\times\Large{y}}L(y,f(x))P(x,y)dxdy$$ 这是理论上$f(X)$关于联合分布$P(X,Y)$的平均意义下的损失，称为风险函数（risk function）或期望损失（expected loss）。学习的目标就是选择期望风险最小的模型。 给定一个训练数据集 $$T={(x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N)}$$ 模型$f(X)$关于训练数据集的平均损失称为经验风险(empirical risk)或经验损失(empirical loss)，记作$R_{emp}$: $$R_{emp}(f)=\frac{1}{N}\sum_{i=1}^N{L(y_i,f(x_i))}$$ 期望风险$R_{exp}(f)$是模型关于联合分布的期望损失，经验风险$R_{emp}(f)$是模型关于训练样本集的平均损失。根据大数定律，当样本容量$N$趋于无穷时，经验风险$R_{emp}(f)$趋于期望风险$R_{exp}(f)$。但是现实中训练样本数目有限，甚至很小，所以用经验风险估计期望风险常常并不理想，需要对经验风险进行一定的矫正。 经验风险最小化与结构风险最小化在假设空间、损失函数以及训练数据集确定的情况下，经验风险函数式就可以确定。通过最小化经验风险就可以得到最优的模型。根据这一策略，按照经验风险最小化求最优模型就是求解最优化问题： $$min_{f\in{F}}\frac{1}{N}\sum_{i=1}^N{L(y_i,f(x_i))}$$ 其中，$F$是假设空间。 当样本容量足够大时，经验风险最小化能保证有很好的学习效果，在现实中被广泛采用。当样本容量很小的时候，经验风险最小化学习的效果就不一定很好，会产生过拟合现象。 结构风险最小化(structural risk minimization,SRM)是为了防止过拟合而提出的策略。结构风险最小化等价于正则化(regularization)。结构风险在经验风险上加上表示模型复杂度的正则项(regularizer)或惩罚项(penalty term)。在假设空间、损失函数以及训练数据集确定的情况下，结构风险的定义是： $$R_{srm}(f)=\frac{1}{N}\sum_{i=1}^N{L(y_i,f(x_i))}+\lambda{J(f)}$$ 其中$J(f)$为模型的复杂度，是定义在假设空间$F$上的泛函。模型$f$越复杂，复杂度$J(f)$就越大。也就是说，复杂度表示了对复杂模型的惩罚。$\lambda\geq{0}$是系数，用以权衡经验风险和模型复杂度。结构风险小需要经验风险和模型复杂度同时小。 结构风险最小化的策略认为结构风险最小的模型就是最优的模型。所以求最优模型，就是求解以下问题： $$min_{f\in{F}}R_{srm}(f)=min_{f\in{F}}{\frac{1}{N}\sum_{i=1}^N{L(y_i,f(x_i))}+\lambda{J(f)}}$$ 算法算法是指学习模型的具体计算方法。统计学习基于训练数据集，根据学习策略，从假设空间中选择最优模型，最后考虑用什么样的计算方法求解最优模型。 这时，统计学习问题归结为最优化问题，统计学习的算法称为求解最优化问题的算法。统计学习方法之间的不同，主要来自其模型、策略、算法的不同。确定里模型、策略、算法，统计学习的方法也就确定了。]]></content>
      <categories>
        <category>统计学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[统计学习方法概论]]></title>
    <url>%2F2019%2F09%2F09%2F%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E6%A6%82%E8%AE%BA%2F</url>
    <content type="text"><![CDATA[1. 统计学习统计学习（statistical learning）是关于计算机基于数据构建概率统计模型并运用模型对数据进行预测人与分析的一门学科。 $$已有数据\xrightarrow{构建}模型\xrightarrow{预测分析}数据$$ 主要特点 统计学习以计算机与网络为平台，是构建在计算机及网络之上的； 统计学习以数据为研究对象，是数据驱动的学科； 统计学习的目的是对数据进行预测与分析； 统计学习以方法为中心，统计学习方法构建模型并应用模型进行预测与分析； 统计学习是概率论、统计学、信息论、计算理论、最优化理论及计算机科学等多个领域的交叉学科，并且在发展中逐步形成独自的理论体系与方法论。 统计学习的对象是数据，统计学习从数据出发，提取数据的特征，抽取出数据的模型，发现数据中的知识，又回到对数据的分析与预测中。数据形式多样，包括存在于计算机及网络上的各种数字、文字、图像、音频、视频数据以及它们的组合。 统计学习的前提统计学习关于数据的基本假设是同类数据具有一定的统计规律性。这里的同类数据指的是具有某种共同性质的数据。由于它们具有统计规律性，所以可以用概率统计方法来加以处理。 统计学习的目的统计学习用于对数据进行预测和分析，特别是对未知新数据进行预测与分析。对数据的预测可以使计算机更加智能化，或者说使计算机的某些性能得到提升；对数据的分析可以让人们获得新的知识，给人们带来新的发现。 统计学习总的目标就是考虑学习什么样的模型和如何学习模型，以使模型能对数据进行准确的预测与分析，同时也要考虑尽可能地提升学习效率。 统计学习与机器学习的异同 统计学习和机器学习的界限一直很模糊，有人认为统计学习偏向于理论上的完善，机器学习基于统计学习，并将其延伸到实践中。机器学习旨在于使最准确的预测成为可能，统计学习模型是为推断变量之间的关系而设计的。我认为，统计学习与机器学习主要就是理论与实践的区别。 统计学习的方法统计学习的方法时基于数据构建统计模型从而对数据进行预测与分析。统计学习由监督学习（supervised learning）、非监督学习（unsupervised learning）、半监督学习（semi-supervised learning）和强化学习（reinforcement learning）等组成。 监督学习是从给定的、有限的、用于学习的训练模型（training data）集合出发，假设数据是独立同分布产生的；并且假设要学习的模型属于某个函数的集合，称为假设空间（hypothesis space）；应用某个评价准则（evaluation criterion），从假设空间中选取一个最优的模型，使它对已知训练数据集未知测试数据（test data）在给定的评价准则下有最优的预测；最优模型的选取由算法实现。 统计学习方法包括模型的假设空间、模型选择的准则以及模型学习的算法，称其为统计学习方法的三要素，简称为模型（model）、策略（stratage）和算法（algorithm）。 实现统计学习方法的步骤如下：（1）得到一个有限的训练数据集合；（2）确定包含多有可能的模型的假设空间，即学习模型的集合；（3）确定模型选择的准则，即学习的策略；（4）实现求解最优模型的算法，即学习的算法；（5）通过学习方法选择最优模型；（6）利用学习的最优模型对新数据进行预测或分析。 统计学习研究一般包括统计学习方法（statistical learning method）、统计学习理论（statistical learning theory）以及统计学习应用三个方面。 2. 监督学习监督学习（supervised learning）的任务是学习一个模型，是模型能够对任意给定的输入，对其相应的输出做出一个好的预测。 基本概率输入空间、特征空间与输出空间在监督学习中，将输入与输出所有可能取值的集合分别称为输入空间与输出空间。输入与输出空间可以是有限元素的集合，也可以是整个欧氏空间。输入和输出空间可以是同一空间，也可以不是同一空间；通常输出空间小于输入空间。每一个具体的输入都是一个实例（instance），通常由特征向量（feature vector）表示。所有的特征向量存在的空间称为特征空间（feature space）。 联合概率分布监督学习假设输入与输出的随机变量$X$和$Y$遵循联合概率分布$P(X,Y)$。$P(X,Y)$表示分布函数，或分布密度函数。 假设空间监督学习的目的在于学习一个由输入到输出的映射，这一映射由模型来表示。换句话说，学习的目的就是找到最满足映射关系的模型。模型属于由输入空间到输出空间的映射的集合，这个集合就是假设空间（hypothesis space）。监督学习的模型可以是概率模型或非概率模型，由条件概率分布$P(Y/X)$或决策函数（decision function）$Y=f(X)$表示。 问题的形式化监督学习利用训练数据集学习一个模型，再用模型对测试样本集进行预测（prediction）。由于在这个过程中需要训练数据集，而训练数据集往往是通过人工标注的，所以统称为监督学习。监督学习分为学习和预测两个过程，由学习系统和预测系统完成。 首先给定一个训练数据集： $$T={(x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N)}$$ 其中$(x_i,y_i),i=1,2,\cdots,N$，称为样本或样本点。$x_i\in\chi\subseteqq{R^n}$是输入的观测值，也称为输入或实例，$y_i\subseteqq\LARGE{y}$是输出的观测值，也称为输出。 监督学习中，假设训练数据和测试数据都是依联合概率分布$P(X,Y)$独立同分布产生的。在学习过程中，学习系统利用给定的训练数据集，通过学习（或训练）得到一个模型，表示为条件概率分布$\hat{P}(Y/X)$或决策函数$Y=\hat{f}(X)$。条件概率分布$\hat{P}(Y/X)$或决策函数$Y=\hat{f}(X)$描述输入与输出随机变量之间的映射关系。在预测过程中，预测系统对于给定的测试样本集合中的输入$x_{N+1}$，由模型$y_{N+1}=argmax_{y_{N+1}}\hat{P}(y_{N+1}/x_{N+1})$或$y_{N+1}=\hat{f}(x_{N+1})$给出相应的输出$y_{N+1}$。其中$y_{N+1}=argmax_{y_{N+1}}\hat{P}(y_{N+1}/x_{N+1})$表示的是对于给定的$x_{N+1}$使得$\hat{P}(y_{N+1}/x_{N+1})$最大时对应的$y_{N+1}$的取值。在学习的过程中，学习系统（也就是学习算法）试图通过训练数据集中的样本$(x_i,y_i)$带来的信息学习模型。具体来说，对于输入$x_i$，一个具体的模型$y=f(x)$可以产生一个输出$f(x_i)$，而训练数据集中对应的输出是$y_i$，如果这个模型有很好的预测能力，那么训练样本输出$y_i$就应该和模型的输出$f(x_i)$尽可能接近。]]></content>
      <categories>
        <category>统计学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[句子分块]]></title>
    <url>%2F2019%2F04%2F19%2F%E5%8F%A5%E5%AD%90%E5%88%86%E5%9D%97%2F</url>
    <content type="text"><![CDATA[句子分块分块也称为浅层分析，它基本上是识别句子部分和短语(如名词短语)。 词性标注告诉你单词是名词，动词，形容词等，但它并没有给你任何关于句子中句子或短语结构的线索。有时除了单词的词性，自然语言处理任务需要获取更多信息，这是就需要对句子进行解析，从中获得完整的解析树。PyRATA Python nltk.RegexpParser() Examples 1234567891011121314151617181920212223242526272829303132def prepareForNLP(text): sentences = nltk.sent_tokenize(text) sentences = [nltk.word_tokenize(sent) for sent in sentences] sentences = [nltk.pos_tag(sent) for sent in sentences] return sentencesdef chunk(sentence): chunkToExtract = &quot;&quot;&quot; NP: &#123;&lt;NNP&gt;*&#125; &#123;&lt;DT&gt;?&lt;JJ&gt;?&lt;NNS&gt;&#125; &#123;&lt;NN&gt;&lt;NN&gt;&#125;&quot;&quot;&quot; grammar = r&quot;&quot;&quot; NP: &#123;&lt;DT|JJ|NN.*&gt;+&#125; # Chunk sequences of DT, JJ, NN PP: &#123;&lt;IN&gt;&lt;NP&gt;&#125; # Chunk prepositions followed by NP VP: &#123;&lt;VB.*&gt;&lt;NP|PP|CLAUSE&gt;+&#125; # Chunk verbs and their arguments CLAUSE: &#123;&lt;NP&gt;&lt;VP&gt;&#125; # Chunk NP, VP &#125;&lt;[\.VI].*&gt;+&#123; # chink any verbs, prepositions or periods &quot;&quot;&quot; parser = nltk.RegexpParser(grammar) result = parser.parse(sentence) print &quot;result.label():&quot;, result.label() for subtree in result.subtrees(): t = subtree t = &apos; &apos;.join(word for word, pos in t.leaves()) print(t)if __name__ == &apos;__main__&apos;: example_sent = &quot;A man with a red helmet stands on a small moped on a dirt road .&quot;.lower() sentences = prepareForNLP(example_sent) for sentence in sentences: chunk(sentence) 输出：123456789a man with a red helmet stands on a small moped on a dirt road .a manwith a red helmeta red helmetstands on a small moped on a dirt roadon a small mopeda small mopedon a dirt roada dirt road 相关链接名词短语的分块：NP Chunking (State of the art)) 词性标记集英语标记集]]></content>
      <categories>
        <category>科研</category>
      </categories>
      <tags>
        <tag>自然语言处理</tag>
        <tag>chunk</tag>
        <tag>NLTK</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[阿里巴巴实习生面试]]></title>
    <url>%2F2019%2F04%2F17%2F%E9%98%BF%E9%87%8C%E5%B7%B4%E5%B7%B4%E5%AE%9E%E4%B9%A0%E7%94%9F%E9%9D%A2%E8%AF%95%2F</url>
    <content type="text"><![CDATA[阿里巴巴暑期实习，官网内推投的成都蚂蚁金服，经过漫长等待和催促终于等来了面试。 一面（48min）自我介绍Java并发 一个之前没注意的问题：synchronized关键字可以修饰类吗？ Spring bean的生命周期和作用域IOC有什么用解耦 MySQL数据库 索引 索引的优缺点 锁 蚂蚁金服干啥了解吗安全 蚂蚁金服的对手有哪些微信支付、京东金融 蚂蚁金服和对手的优势和劣势幂等律一个请求与多个请求等效 一个用户的同一请求（比如充值操作）由于网络问题等多次发送给服务端，怎么实现幂等？]]></content>
      <categories>
        <category>工作</category>
      </categories>
      <tags>
        <tag>实习</tag>
        <tag>面试</tag>
        <tag>阿里巴巴</tag>
        <tag>蚂蚁金服</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Stacked Cross Attention for Image-Text Matching]]></title>
    <url>%2F2019%2F04%2F15%2FStacked-Cross-Attention-for-Image-Text-Matching%2F</url>
    <content type="text"><![CDATA[这篇文章做了什么？通过对图像中的对象或者突出内容与句子中的单词进行潜在语义对齐，使得图文匹配过程能够捕获到视觉与语言之间细粒度的相互关系，使得图文匹配更具有可解释性。目的是将单词和图像区域映射到同一嵌入空间（common embedding space）从而推断整个图像与句子之间的相似度。 相比于已有的方法有什么优点？已有方法： 简单聚合所有可能的区域以及单词对之间的相似度，却没有关注区分单词和区域之间的重要性； 使用多步骤的注意力过程来捕获数量有限且缺乏一定解释性的语义对齐。 这篇文章使用提出的Stacked Cross Attention去发现在图像区域和单词之间的所有潜在对齐，从而计算图文相似度。已有方法通过执行固定步骤的注意力推理，从而在一个时刻只能发现有限的语义对齐，而Stacked Cross Attention可以同时发现所有可能的语义对齐。由于语义对齐的数量随着不同的图像和句子而变化，因此Stacked Cross Attention方法推断出的对应关系更加全面，从而使图像文本匹配更具可解释性。 这篇文章是怎么做的？ 利用自底向上的注意力机制检测图像区域，并提取图像区域的特征； 将句子中的单词及其句子上下文映射为特征向量； 应用Stacked Cross Attention通过对齐图像区域和单词特征来推断图文相似度。 这篇文章的损失函数关注每一个Batch中最负面的图文对（也就是最不匹配的图文对）。所以对于给定的正样例对$(I,T)$，那么最负样例对定义为$\hat{I}h=argmax{m\neq{I}}S(m,T)$以及$\hat{T}h=argmax{d\neq{T}}S(I,d)$。所以这篇文章定义损失函数如下： $$l_{hard}(I,T)=[\alpha-S(I,T)+S(I,\hat{T}h)]++[\alpha-S(I,T)+S(\hat{I}h,T)]+$$ 怎么文章题目中的Stacked Cross Attention？Stacked Cross Attention通过两个阶段来关注图像和文本的上下文信息。第一个阶段，给定一个图像和一个句子，关注每个图像区域对应的句子中的单词，并将每个图像区域与来自句子的受关注信息进行比较，以确定图像区域的重要性（比如图像区域在句子中是否被提到）。第二阶段，与第一阶段相似的，根据每个单词对应的图像区域来决定每个单词的重要性（也就是对每个单词的关注度）。 文章的实验结果展示这篇文章在Flickr30K以及MS-COCO数据集上进行实验，结果如下：]]></content>
      <categories>
        <category>科研</category>
      </categories>
      <tags>
        <tag>论文阅读</tag>
        <tag>神经网络</tag>
        <tag>图像识别</tag>
        <tag>图文匹配</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MyIsam存储引擎]]></title>
    <url>%2F2019%2F04%2F10%2FMyIsam%E5%AD%98%E5%82%A8%E5%BC%95%E6%93%8E%2F</url>
    <content type="text"><![CDATA[Spring Boot的@Transactional注解对MyISAM存储引擎生效的问题？ 未解决问题，一脸懵逼。。。。 其中，数据表对应的实体对象如下，1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556import javax.persistence.Entity;import javax.persistence.GeneratedValue;import javax.persistence.GenerationType;import javax.persistence.Id;@Entitypublic class User &#123; @GeneratedValue(strategy = GenerationType.AUTO) @Id private Long id; private String name; private String password; public User() &#123; &#125; public User(String name, String password) &#123; this.name = name; this.password = password; &#125; @Override public String toString() &#123; return &quot;User&#123;&quot; + &quot;id=&quot; + id + &quot;, name=&apos;&quot; + name + &apos;\&apos;&apos; + &quot;, password=&apos;&quot; + password + &apos;\&apos;&apos; + &apos;&#125;&apos;; &#125; public Long getId() &#123; return id; &#125; public void setId(Long id) &#123; this.id = id; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public String getPassword() &#123; return password; &#125; public void setPassword(String password) &#123; this.password = password; &#125;&#125; 使用Spring Data Jpa进行数据操作，代码如下：123456789101112131415import org.springframework.data.jpa.repository.JpaRepository;import org.springframework.data.jpa.repository.Modifying;import org.springframework.data.jpa.repository.Query;import org.springframework.stereotype.Repository;@Repository(&quot;user_repository&quot;)public interface UserRepository extends JpaRepository&lt;User, Long&gt; &#123; Integer deleteUserById(Long id); @Modifying(clearAutomatically = true) @Query(value = &quot;update User set name=?2,password=?3 where id=?1&quot;) Integer updateUserById(Long id,String name,String password);&#125; 业务逻辑层代码如下：123456789101112131415161718192021222324252627282930313233343536import org.springframework.beans.factory.annotation.Autowired;import org.springframework.stereotype.Service;import org.springframework.transaction.annotation.Transactional;@Service(&quot;uService&quot;)public class UserService &#123; @Autowired UserRepository userRepository; @Transactional(rollbackFor = Exception.class) public Integer deleteUserById(Long id)&#123; Integer i = userRepository.deleteUserById(id); if(id.equals(718L))&#123; throw new IllegalArgumentException(&quot;删除714错误，回滚&quot;); &#125; return i; &#125; @Transactional(rollbackFor = &#123;IllegalArgumentException.class&#125;) public User saveUser(User user)&#123; User u = userRepository.save(user); if(u.getName()==&quot;yang1&quot;)&#123; throw new IllegalArgumentException(&quot;存入yang，导致回滚&quot;); &#125; return u; &#125; @Transactional(rollbackFor = &#123;IllegalArgumentException.class&#125;) public Integer updateUserById(User user)&#123; Integer i = userRepository.updateUserById(user.getId(),user.getName(),user.getPassword()); return i; &#125;&#125; 最后是测试代码，123456789101112131415161718192021222324252627282930@RunWith(SpringRunner.class)@SpringBootTestpublic class UserServiceTest &#123; @Autowired UserService userService; private final Logger LOGGER = LoggerFactory.getLogger(this.getClass()); @Test public void deleteUserById() &#123; Integer i = userService.deleteUserById(718L); LOGGER.error(&quot;delete by user id successful: &quot;+ i); &#125; @Test public void saveUser()&#123; User user = new User(&quot;yang&quot;,&quot;adasdsa&quot;); User u = userService.saveUser(user); LOGGER.error(u.toString()); &#125; @Test public void updateUser()&#123; User user = new User(&quot;yang&quot;,&quot;adasdsa&quot;); user.setId(714L); int i = userService.updateUserById(user); LOGGER.error(&quot;update by user id successful: &quot;+ i); &#125;&#125; 在测试删除用户（deleteUserById）以及保存用户（saveUser）时，发现抛出异常会导致回滚，但是我的数据库引擎是MyIsam，讲道理不应该支持事务。]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>MyIsam</tag>
        <tag>数据库</tag>
        <tag>问题</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[代理模式]]></title>
    <url>%2F2019%2F04%2F08%2F%E4%BB%A3%E7%90%86%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[代理是一种设计模式，提供了对目标对象的间接访问方式，即通过代理访问目标对象。代理模式的作用：控制和管理访问。 静态代理在程序运行前就已经存在对应的代理类，那么这种代理方式就称之为静态代理。由于这种代理方式下的代理类往往是程序员自己在Java代码中定义的，当我们需求发生变化的时候，代理类以及目标类都需要进行修改维护，不够灵活。但是静态代理也有优点，就是在不对目标对象进行修改的前提下就可以对目标对象进行功能扩展和拦截。下面举个简单的例子，在CS（客户端\服务器）主从结构的网络架构中，一般服务器会对客户端发送而来的请求进行过滤和记录，这个时候就可以使用代理类来实现上述功能。 首先，我们定义服务端顶层接口： 123456 /** * 顶层接口，目标类和代理类都需要实现顶层接口 */interface ServerInterface&#123; public void processRequest(String request);&#125; 其次，定义服务类： 123456789 /** * 目标类 */class Server implements ServerInterface&#123; @Override public void processRequest(String request) &#123; System.out.println(&quot;process request&quot;); &#125;&#125; 最后定义服务代理类，服务类和代理类都实现服务顶层接口：12345678910111213141516171819202122232425262728293031323334353637383940414243/** * 代理类 */class ServerProxy implements ServerInterface&#123; ServerInterface serverInterface; public ServerProxy(ServerInterface serverInterface) &#123; this.serverInterface = serverInterface; &#125; /** * 过滤来自客户端的不合理请求 * @param request * @return */ public boolean filter(String request)&#123; System.out.println(&quot;filter request&quot;); return true; &#125; /** * 记录来自客户端的请求 */ public void logBefore()&#123; System.out.println(&quot;log before&quot;); &#125; /** * 记录返回给客户端的结果 */ public void logReturn()&#123; System.out.println(&quot;log return&quot;); &#125; @Override public void processRequest(String request) &#123; logBefore(); if(!filter(request))&#123; serverInterface.processRequest(request); &#125; logReturn(); &#125;&#125; 从上面可以看出，当我们的需求改变的时候，比如说我们不仅需要过滤请求、日志记录，还需要进行客户端登录时，那么就需要在代理类中添加登录服务器的方法logIn()，并且给processRequest方法中添加登录方法，而实际情况中服务器中处理请求的方法processRequest成千上万，这个时候程序员就需要给一个个processRequest方法添加logIn操作，可以看到修改和维护代码代价过大，代码也不够简洁。 动态代理JDK原生动态代理动态代理中，代理类是在运行时通过Java中的反射机制动态生成的。有兴趣的可以去看看Java中ProxyClassFactory类以及ProxyGenerator类动态生成代理类的源码。 对于应用程序员而言，主要使用InvocationHandler类以及Proxy.newProxyInstance()实现动态代理。我们还是以上述客户端/服务器网络架构为例。 顶层的服务接口ServerInterface不变，目标类Server也不用改变，只用实现InvocationHandler接口如下：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354/** * 日志记录类 */class Logger &#123; /** * 记录来自客户端的请求 */ public void logBefore() &#123; System.out.println(&quot;log before&quot;); &#125; /** * 记录返回给客户端的结果 */ public void logReturn() &#123; System.out.println(&quot;log return&quot;); &#125;&#125;/** * 动态代理 */class ServerDynamicProxy implements InvocationHandler &#123; Object target; Object prozy; public ServerDynamicProxy(Object target, Object prozy) &#123; this.target = target; this.prozy = prozy; &#125; /** * 动态绑定，返回代理对象 * * @return */ public Object bind() &#123; return Proxy.newProxyInstance(target.getClass().getClassLoader() , target.getClass().getInterfaces(), this); &#125; @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123; Class prozyClass = proxy.getClass(); Method beforeMethod = prozyClass.getMethod(&quot;logBefore&quot;); Method returnMethod = prozyClass.getMethod(&quot;logReturn&quot;); beforeMethod.invoke(prozy); method.invoke(this.target, args); returnMethod.invoke(prozy); return null; &#125;&#125; 以上就是动态代理的使用，当测试的时候只需要在主函数中调用bind()方法返回代理类就可以了，当然需要进行强制类型转换，从Object转换成目标类。如果想看见动态代理生成的代理类，可以使用System.getProperties().put(&quot;sun.misc.ProxyGenerator.saveGeneratedFiles&quot;,&quot;true&quot;); CGLIB动态代理CGLIB(Code Generation Library)是一个基于ASM的字节码生成库，它允许在运行时对字节码进行修改和动态生成。CGLIB通过继承方式实现代理，所以当目标类没有实现接口时，就无法使用JDK代理，只能使用CGLIB实现动态代理。示例代码如下：1234567891011121314151617181920212223242526272829303132333435363738394041/** * 目标类 */public class Server &#123; public void processRequest(String request) &#123; System.out.println(&quot;process request&quot;); &#125;&#125;/** * 代理类 */public class ServerMethodInterceptor implements MethodInterceptor &#123; private Object target;//业务类对象，供代理方法中进行真正的业务方法调用 public ServerMethodInterceptor(Object target) &#123; this.target = target; &#125; //相当于JDK动态代理中的绑定 public Object getInstance() &#123; Enhancer enhancer = new Enhancer(); //创建加强器，用来创建动态代理类 enhancer.setSuperclass(this.target.getClass()); //为加强器指定要代理的业务类（即：为下面生成的代理类指定父类） //设置回调：对于代理类上所有方法的调用，都会调用CallBack，而Callback则需要实现intercept()方法进行拦 enhancer.setCallback(this); // 创建动态代理类对象并返回 return enhancer.create(); &#125; @Override public Object intercept(Object o, Method method, Object[] objects, MethodProxy methodProxy) throws Throwable &#123; // log before System.out.println(&quot;log before&quot;); Object result = methodProxy.invokeSuper(o,objects); // log return System.out.println(&quot;log return&quot;); return result; &#125;&#125;]]></content>
      <categories>
        <category>设计模式</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>动态代理</tag>
        <tag>静态代理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[字节跳动实习生面试]]></title>
    <url>%2F2019%2F03%2F30%2F%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8%E5%AE%9E%E4%B9%A0%E7%94%9F%E9%9D%A2%E8%AF%95%2F</url>
    <content type="text"><![CDATA[一面 垃圾回收讲一下； java内存模型； 垃圾回收算法（如何确定回收对象（引用计数法、可达性算法）、回收算法（标记回收、标记整理、复制））； 垃圾收集器； MySQL数据库讲一下；数据引擎、索引类型、索引方法（B+树、Hash） 手写代码：根据前序遍历和中序遍历还原二叉树，根据获得的前序遍历和中序遍历结果输出一颗二叉树（输出后续遍历）（我问可以IDE吗？尽量别用）二面 手写代码： 123456单链表操作： 输入：奇数位升序，偶数位降序 1 -&gt; 80 -&gt; 4 -&gt; 60 -&gt; 6 -&gt; 40 -&gt; 7 -&gt; 2 输出：升序单链表 1 -&gt; 2 -&gt; 4 -&gt; 6-&gt; 7 -&gt; 40 -&gt; 60 -&gt; 80 不允许使用其他数据结构。 进程和线程的区别 HashMap TCP/UDP三面-两道编程题： 假定一张表的数据格式为 id,name,parentId，表的数据不大，1000条以内，得到这些数据的树型结构 输入：List ， 输出：Node（手写代码） 有2个文件，分别是A(3t大小)，B(2t)大小，A文件的组织形式为 uid, username,B文件的组织形式为 uid, age,找出A、B文件交集的数据放入一个文件，文件的数据格式为uid,username,age（讲讲思路）]]></content>
      <categories>
        <category>工作</category>
      </categories>
      <tags>
        <tag>实习</tag>
        <tag>字节跳动</tag>
        <tag>面试</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Deep Residual Learning for Image Recognition]]></title>
    <url>%2F2019%2F03%2F06%2FDeep-Residual-Learning-for-Image-Recognition%2F</url>
    <content type="text"><![CDATA[神经网络越深就越难训练，作者提出一种残差学习框架来降低网络训练的难度。]]></content>
      <categories>
        <category>科研</category>
      </categories>
      <tags>
        <tag>论文阅读</tag>
        <tag>神经网络</tag>
        <tag>图像识别</tag>
        <tag>残差网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ZooKeeper-分布式应用程序协调服务]]></title>
    <url>%2F2019%2F03%2F06%2FZooKeeper-%E5%88%86%E5%B8%83%E5%BC%8F%E5%BA%94%E7%94%A8%E7%A8%8B%E5%BA%8F%E5%8D%8F%E8%B0%83%E6%9C%8D%E5%8A%A1%2F</url>
    <content type="text"><![CDATA[参考链接 基本概念Apache ZooKeeper is an effort to develop and maintain an open-source server which enables highly reliable distributed coordination. Apache ZooKeeper致力于开发和维护一个支持高度可靠的分布式协调的开源服务器。Zookeeper是一个典型的分布式数据一致性解决方案，分布式应用程序可以基于Zookeeper实现诸如数据发布/订阅、负载均衡、命名服务、分布式协调/通知、集群管理、Master选举、分布式锁和分布式队列等功能。 Znode在ZooKeeper中，“节点”分为两种类型： 第一类是指构成集群的机器，称之为机器节点； 第二类则是指数据模型中的数据单元，我们称之为数据节点-Znode。 ZooKeeper将所有的数据存储在内存中，数据模型是一棵树（Znode Tree） ZooKeeper集群介绍介绍ZooKeeper集群中的所有机器通过Leader选举过程来选定一台称为“Leader”的机器，Leader既可以为客户端提供写服务又能提供读服务。出Leader外，Follower和Observer只能提供读服务。 Follower与Observer的唯一区别在于Observer机器不参与Leader的选举过程，也不参与写操作的“过半写成功”策略，因此Observer机器可以在不影响写性能的情况下提升集群的读性能。 ZAB协议ZAB（ZooKeeper Atomic Broadcast，ZooKeeper原子广播）协议是为分布式协调服务ZooKeeper专门设计的一种支持崩溃恢复的原子广播协议。 在ZooKeeper中，主要依赖ZAB协议来实现分布式数据一致性，基于该协议ZooKeeper实现了一种主备模式的系统架构来保持集群中各个副本之间的数据一致性。 ZAB协议包括两种基本模式，分别是崩溃恢复和消息广播。当整个服务框架在启动过程中，或是当Leader服务器出现网络中断、崩溃退出与重启等异常情况是，ZAB协议就会进入恢复模式并选举出新的Leader服务器。当选举产生的新的Leader服务器，同时集群中已经有过半的机器与新Leader服务器完成状态同步之后，ZAB协议就会退出恢复模式。其中，所谓的状态同步是指数据同步，也就是保证集群中过半的机器与该Leader服务器的数据状态一致。当集群中已经有过半的Follower服务器完成与Leader服务器的状态同步，那么整个服务框架就可以进入消息广播模式了。当一台同样遵守ZAB协议的服务器启动后加入到集群中时，如果此时集群中已经存在一个Leader服务器在负责消息广播，那么新加入的服务器就会自觉地进去数据恢复模式：找到Leader所在的服务器，并与其进行数据同步，然后一起参与到消息广播流程中去。 正如上文所述，ZooKeeper被设计成只允许唯一的Leader服务器进行事务请求的处理，Leader服务器在接收到客户端的事务请求后，会生成对应的事务提案并发起一轮广播协议。如果集群中的其他机器接收到客户端事务请求，那么这些非Leader服务器会首先将这个事务转发给Leader服务器。 应用在我们的智慧教育示范应用中，主要是使用Kafka收集用户日志，而ZooKeeper就担任了服务生产者和消费者的注册中心。服务生产者将自己提供的服务注册到ZooKeeper中心，服务消费者在进行服务调用的时候先到ZooKeeper中查找服务，获取到服务生产者的详细信息之后，再去调用服务生产者的内容和数据。]]></content>
      <categories>
        <category>ZooKeeper</category>
      </categories>
      <tags>
        <tag>开源软件</tag>
        <tag>ZooKeeper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[条件生成对抗网络的一系列问题]]></title>
    <url>%2F2019%2F03%2F05%2F%E6%9D%A1%E4%BB%B6%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C%E7%9A%84%E4%B8%80%E7%B3%BB%E5%88%97%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[条件生成对抗网络中生成器的输入随机噪声z，文本特征$\phi(t)$作为条件，噪声输入有什么用？参考链接：https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/152 其实没有噪声输入也可以，但是有时候需要同一个文本输入需要生成的样本更加多样，这时候随机噪声就有用了。]]></content>
      <categories>
        <category>科研</category>
      </categories>
      <tags>
        <tag>论文阅读</tag>
        <tag>生成对抗网络</tag>
        <tag>问题</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据库范式]]></title>
    <url>%2F2019%2F03%2F04%2F%E6%95%B0%E6%8D%AE%E5%BA%93%E8%8C%83%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[第一范式（1NF）关系模型中实体的每个属性都是原子属性，即元组在每个属性上的取值是不可分的。也就是说这些属性的取值是单一的，不是集合、数组等非原子数据。当实体中的某个属性都多个值时，必须将该属性拆分为多个原子属性。 第二范式（2NF）关系模型中的所有非主属性都完全依赖于所有的候选键。2NF其实就是在1NF的基础上消除那些非主属性对所有候选键的部分函数依赖。例如，关系（学号，课程号，教师，成绩，学院）中，教师部分依赖于课程号，因此这个关系不满足第二范式，所以需要把上述关系拆分成（学号，课程号，成绩）和（课程号，教师，学院）。 第三范式（3NF）关系模型中的非主属性既不部分函数依赖也不传递函数依赖于关系中的所有候选键。也就是说3NF是在2NF的基础上消除了非主属性对候选键的传递依赖关系。 还是上面的例子，关系（课程号，教师，学院）中存在传递依赖关系课程号-&gt;教师，以及教师-&gt;学院，因此不满足3NF。所以将其分解为（课程号，教师）和（教师，学院）。 巴斯-科德范式（BCNF）对于关系R上的任何非平凡函数依赖X-&gt;Y都有X必包含R的某个候选键，那么就称关系R满足BCNF。BCNF是在第三范式的基础上继续消除主属性对于键的部分依赖和传递依赖。 举个例子来说，关系（课程名，班级，教师）中假定多名教师承担多个班级的教学任务，每个教师仅承担一个课程的教学任务，同时可以给多个班级上课，那么可以知道有函数依赖 12(课程名，班级)-&gt;(教师)(教师)-&gt;(课程名) 然而，上述关系中的主键是(课程名，班级)或者(教师，班级)。所以(教师)-&gt;(课程名)就是主属性对候选键的部分依赖，不满足BCNF。 第四范式（4NF）对于关系R中的非平凡多值依赖X-&gt;-&gt;Y，X必包含R的某个候选键，则称满足4NF。]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>数据库</tag>
        <tag>MySQL</tag>
        <tag>范式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[垃圾收集器]]></title>
    <url>%2F2019%2F02%2F28%2F%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E5%99%A8%2F</url>
    <content type="text"><![CDATA[垃圾收集算法是内存回收的方法论，垃圾收集器是垃圾回收的具体实现。在JDK1.7 Update 14之后的HotSpot虚拟机所包含的收集器如下图所示： Serial收集器Serial收集器是最基本、发展历史最悠久的收集器。这是一个单线程的收集器，但它的“单线程”的意义并不仅仅说明它只会使用一个CPU或者一条收集线程去完成垃圾收集工作，更重要的是在它进行垃圾收集时，必须暂停其他所有的工作线程，直到它收集结束。Serial收集器运行示意图如下： 也就是说，虚拟机在使用Serial垃圾收集器时会在用户不可见的情况下把所有用户正常工作的线程暂停，这种“Stop The World”操作在很多程序中是难以接受的，带给用户不良体验。举个例子“你妈妈在打扫房间的时候，肯定也会让你老老实实待在椅子上或者房间外待着，如果她一边打扫，你一边丢垃圾，这房间还能打扫完？你妈妈不会崩溃？”，因此看起来“Stop The World”操作情有可原。但是，从JDK1.3开始，HotSpot虚拟机开发团队一直在努力消除或减少工作线程因内存回收而导致的停顿，从Serial收集器到Parallel收集器，再到Concurrent Mark Sweep（CMS）乃至GC收集器最前沿的成果Garbage First（G1）收集器，用户线程的停顿时间在不断缩短，但是仍然没有办法完全消除垃圾回收带来的停顿。Serial收集器虽然很老，可以说很陈旧，但是目前为止它依旧是虚拟机运行在Client模式下的默认新生代收集器。那是因为Serial收集器简单而高效，对于限定单个GPU的情况而言，Serial收集器由于没有线程交互的开销，专心进行垃圾收集自然可以获得最高的单线程收集效率。 ParNew收集器ParNew收集器就是Serial收集器的多线程版本，除了使用多线程进行垃圾收集外，其余行为包括Serial收集器的所有可用控制参数、收集算法、Stop The World、对象分配规则、回收策略等都与Serial收集器完全一样。ParNew收集器进行垃圾收集的示意图如下： ParNew收集器除了多线程收集外，其他与Serial收集器相比没有太多创新之处，但它却是虚拟机运行在Server模式下的首选新生代收集器，其中一个与性能无关的重要原因是除了Serial收集器外，目前只有它能与CMS收集器配合工作。 Parallel Scavenge收集器Parallel Scavenge收集器也是新生代收集器，并且使用复制算法收集内存，同时还是并行的多线程收集器。这些特点都和ParNew收集器一样，但是Parallel Scavenge收集器的目标在于达到一个可控制的吞吐量（Throughput），所谓吞吐量就是CPU用于运行用户代码的时间与CPU总消耗时间的比值，即吞吐量=运行用户代码/(运行用户代码+垃圾收集时间)。这与CMS等收集器的目标不同，它们目标是尽可能地缩短垃圾收集时间用户线程的停顿时间。停顿时间越短就越适合需要与用户交互的程序，良好的响应速度能提升用户体验，而高吞吐量则可以高效率地利用CPU时间，尽快完成程序的运算任务，主要适合在后台运算而不需要太多的交互任务。Parallel Scavenge收集器提供两个参数用于精准控制吞吐量，分别是控制最大垃圾收集时间的-XX:MaxGCPauseMillis参数以及直接设置吞吐量大小的-XX:GCTimeRatio参数。MaxGCPauseMillis参数允许的值是一个大于0的毫秒数，收集器将尽可能在规定时间内完成垃圾收集操作。但是不要认为这个参数越小，系统的垃圾收集速度就会越快，GC停顿时间的缩短是以牺牲吞吐量和新生代空间来换取的：系统把新生代调小一点，收集300M新生代肯定比收集500M新生代快，这也将导致垃圾收集更频繁，原来每20s收集一次、一次100ms，现在每10s收集一次、一次70ms。停顿时间的确下降了，但吞吐量也降下来了。GCTimeRatio参数的值应当是一个大于0小于100的整数，如果把这个参数设置为19，那么允许最大GC时间就占总时间的5%（即1/(1+19)）。Parallel Scavenge收集器还有一个参数-XX:+UseAdaptiveSizePolicy，这个参数打开之后就不需要手动指定新生代的大小（-Xmm）、Eden与Survivor区的比例（-XX:SurvivorRatio）、晋升老年代对象大小（-XX:PretenureSizeThreshold）等参数了，虚拟机会根据当前系统的运行情况收集性能监控信息，动态调整这些参数以提供最适合的停顿时间或者最大吞吐量，这种调节方式称为GC自适应的调节策略（GC Ergonomics）。 Serial Old收集器Serial Old是Serial收集器的老年代版本，它同样是一个单线程收集器，使用“标记-整理”算法，这个收集器的主要意义也是在于给Client模式下的虚拟机使用。在Server模式下，它有两大用途： 在JDK1.5以及之前的版本中与Parallel Scavenge收集器搭配使用； 作为CMS收集器的后备方案，在并发收集发生Concurrent Mode Failure时使用。 Parallel Old收集器Parallel Old收集器是Parallel Scavenge收集器的老年代版本，使用多线程和“标记-整理”算法。在JDK1.6之前Parallel Scavenge只能与Serial Old收集器配合使用，由于老年代使用单线程收集内存无法充分利用服务器多CPU的处理能力，这个组合就导致无法达到高吞吐量。在JDK1.6开始提供了Parallel Old收集器和Parallel Scavenge收集器配合使用，从而可以应用在注重吞吐量以及CPU资源敏感的场合。 CMS收集器CMS（Concurrent Mark Sweep）收集器是一种以获取最短回收停顿时间为目标的收集器。从名字（包含“Mark Sweep”）就可以看出来，CMS收集器是基于“标记——清除”算法实现的，它的运作过程相对于前面几种收集器来说更复杂一些，整个过程分为4个步骤，包括： 初始标记（CMS initial mark） 并发标记（CMS concurrent mark） 重新标记（CMS remark） 并发清除（CMS concurrent sweep） 其中，初始标记、重新标记这；两个步骤仍需要“Stop The World”。初始标记是标记一下GC Roots能直接关联到的对象，速度很快，并发标记阶段就是进行GC Roots Tracing的过程，而重新标记阶段是为了修正并发标记阶段因用户程序继续运作而导致标记产生变动的那一部分对象的标记记录，这一阶段的停顿时间一般会比初始标记阶段稍长一些，但远比并发标记阶段的时间短。整个过程中耗时最长的并发标记和并发清除阶段都是与用户线程一起工作，如下图所示： CMS是一款优秀的收集器，它的主要优点是并发收集、低停顿，但是它还有以下三个明显缺点： CMS收集器对CPU资源非常敏感； CMS收集器无法处理浮动垃圾（Floating Garbage），可能出现“Concurrent Mode Failure”失败而导致另一次的Full GC。 CMS收集器是基于“标记-清除”算法实现的，在收集结束会产生大量的空间碎片。为了解决这个问题，CMS收集器提供了一个-XX:+USeCMSCompactAtFullCollection开关参数用于在CMS收集器顶不住要进行Full GC时开启内存碎片的合并整理过程。 G1收集器G1收集器是当前收集技术最前沿的成果之一，它被视为JDK1.7中HotSpot虚拟机的一个重要进化特征。G1是一款面向服务端应用的垃圾收集器，与其他收集器相比，G1具备如下特点： 并行与并发 分代收集 空间整合 可预测的停顿 在G1之前的其他收集器进行收集的范围是整个新生代或者老年代，而G1不再是这样。使用G1收集器时，Java堆的内存布局就与其他收集器有很大区别，它将整个Java堆划分成多个大小相等的独立区域（Region），虽然还保留有新生代和老年代的概念，但是新生代和老年代不再是物理隔离的了，它们都是一部分Region（不需要连续）的集合。]]></content>
      <categories>
        <category>深入理解Java虚拟机</category>
      </categories>
      <tags>
        <tag>垃圾收集器</tag>
        <tag>垃圾收集算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java并发编程基础]]></title>
    <url>%2F2019%2F02%2F27%2FJava%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"><![CDATA[线程现代操作系统在运行一个程序的时候就会创建一个进程。线程是现在操作系统系统调度的最小单元，也称为轻量级进程（Light Weight Process），在一个进程里可以创建多个线程，每个线程都有自己的程序计数器、堆栈和局部变量等属性，并且能够访问内存中共享的变量。 使用多线程的原因 更多的处理器核心：随着处理器上的核心数量越来越多，以及超线程技术的广泛应用，现在的计算机比以往更加擅长并行计算； 更快的响应时间：当业务逻辑过于复杂，使用多线程技术，将对数据一致性要求不高的操作派发给多个线程处理，这样做能更快地处理完一个复杂任务，缩短响应时间，提升用户体验； 更好的编程模型：Java为多线程提供了良好、考究并且一致的编程模型，使开发人员能够更加专注于问题的解决，即为所遇到的问题建立合适的模型，而不是绞尽脑汁地考虑如何将任务多线程化。 线程优先级在Java线程中，通过一个整型成员变量priority来控制优先级，优先级的范围从1~10，在线程构建的时候可以通过setPriority(int)方法修改线程的优先级，默认优先级为5，优先级高的线程分配时间片的数量要多于优先级低的线程。设置线程优先级时，针对频繁阻塞的线程则设置较低的优先级，保证处理器不会被独占。 线程状态Java线程在运行的生命周期可能处于6种不同的状态，在任意给定时刻线程只能处于其中的一种状态： 状态名称 说明 NEW 初始状态，线程被构建，但是还没有调用start()方法 RUNNABLE 运行状态，Java线程将操作系统中的就绪和运行两种状态笼统地称为“运行中” BLOCKED 阻塞状态，表示线程阻塞于锁 WAITING 等待状态，表示线程进入等待状态，进入该状态表示当前线程需要等待其他线程做出一些特定动作（通知或中断） TIME_WAITING 超时等待状态，该状态不同于WAITING，它是可以在指定时间自行返回的 TERMINATED 终止状态，表示当前线程已经执行完毕 Daemon线程Daemon线程是一种支持型线程，因为它主要被用作程序中后台调度以及支持性工作。这意味着，当一个Java虚拟机中不存在非Daemon线程时，Java虚拟机将会退出。可以在线程启动之前通过调用Thread.setDaemon(true)将线程设置为Daemon线程。]]></content>
      <categories>
        <category>Java并发编程</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>多线程</tag>
        <tag>线程</tag>
        <tag>synchronized</tag>
        <tag>volatile</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ubuntu配置shadowsocks科学上网]]></title>
    <url>%2F2019%2F02%2F26%2FUbuntu%E9%85%8D%E7%BD%AEshadowsocks%E7%A7%91%E5%AD%A6%E4%B8%8A%E7%BD%91%2F</url>
    <content type="text"><![CDATA[系统环境 showsocks配置 安装shadowsocks客户端,命令如下 1sudo apt install shadowsocks 查看shadowsocks命令： 1sslocal --help 启动shadowsocks两种方式： 通过设置各个参数，如下 1sslocal -s 11.22.33.44 -p 50003 -k &quot;123456&quot; -l 1080 -t 600 -m aes-256-cfb 或者，直接加载json配置文件 1sslocal -c file_path/shadowsocks.json 启动shadowsocks如下所示： 配置好shadowsocks客户端后，我们介绍一下三种方式设置代理模式。 方式一：不要关闭上面的终端，重新打开一个终端，配置全局变量：使用vim ~/.bashrc添加语句1alias hp=&quot;http_proxy=http://localhost:8123&quot; 使用source ~/.bashrc使更改生效。 测试效果 使用命令hp curl ip.gs查看代理服务器网址，结果如下所示： 可以看出，alias hp=&quot;http_proxy=http://localhost:8123&quot;这句语句的作用就是用命令hp替换http_proxy=http://localhost:8123这一串长命令。 那我们不想每条指令前面都加上命令hp怎么办，这就需要设置全局代理：方式二：为当前窗口设置全局代理命令如下：12export http_proxy=http://localhost:8123 # 当前终端使用代理unset http_proxy # 当前终端取消代理 测试如下所示，可以看出上设置全局代理后，就不需要hp命令了，取消代理（unset）后可以看到ip变成了陕西西安。 方式三：设置当前用户的全局代理在~/.bashrc中添加export http_proxy=http://localhost:8123命令设置当前用户的全局代理。别忘了使用source ~/.bashrc使更改生效。 如下所示， 经过测试，推荐方式二为当前窗口设置全局代理，命令如下：12export http_proxy=http://localhost:8123 # 当前终端使用代理unset http_proxy # 当前终端取消代理 这种方式可以开启一个窗口运行export作为科学上网窗口，而其他窗口依旧正常上网，这样可以节约流量。除此之外，这是我测试的上述三种上网方式中最稳定的一种，建议使用。 总结配置好之后，科学上网的过程，先打开一个终端窗口，运行： 1sslocal -c file_path/shadowsocks.json 再打开新窗口，运行1export http_proxy=http://localhost:8123 我使用R-3D中下载YouTube视频的代码作为测试，运行python download_video.py就可以看到下载视频并保存在当前目录的videos目录下。 示意结果如下，sslocal -c shadowsocks.json启动窗口下方会出现与www.youtube.com连接的日志信息：]]></content>
      <categories>
        <category>工程</category>
      </categories>
      <tags>
        <tag>Ubuntu</tag>
        <tag>shadowsocks</tag>
        <tag>科学上网</tag>
        <tag>代理</tag>
        <tag>proxy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[面向切面编程]]></title>
    <url>%2F2019%2F02%2F21%2F%E9%9D%A2%E5%90%91%E5%88%87%E9%9D%A2%E7%BC%96%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[面向切面编程（Aspect Oriented Programming，AOP）是什么？ AOP是一种编程范式，不是编程语言； AOP解决特定问题，但不能解决所有问题； OOP的补充，而不是其替代。 AOP为什么出现？ 提高代码重用性； 概念分离：分离功能性需求和非功能性需求。将功能性需求从非功能性需求中分离出来。 应用场景 权限控制 缓存控制 事务控制 审计日志 性能监控 分布式追踪 异常处理 Spring AOP的通过代理实现通过DefaultAopProxyFactory.java源码可以看到AOP由jdk和cglib两种方式实现。12345678910111213141516public AopProxy createAopProxy(AdvisedSupport config) throws AopConfigException &#123; if (config.isOptimize() || config.isProxyTargetClass() || hasNoUserSuppliedProxyInterfaces(config)) &#123; Class&lt;?&gt; targetClass = config.getTargetClass(); if (targetClass == null) &#123; throw new AopConfigException(&quot;TargetSource cannot determine target class: &quot; + &quot;Either an interface or a target is required for proxy creation.&quot;); &#125; if (targetClass.isInterface() || Proxy.isProxyClass(targetClass)) &#123; return new JdkDynamicAopProxy(config); &#125; return new ObjenesisCglibAopProxy(config); &#125; else &#123; return new JdkDynamicAopProxy(config); &#125;&#125; Spring AOP的使用在Spring中主要使用注解@Aspect、@Pointcut、@Before、@After、@AfterReturning、@AfterThrowing以及@Around进行面向切面编程。 1234567891011@Retention(RetentionPolicy.RUNTIME)@Target(ElementType.TYPE)public @interface Aspect &#123; /** * Per clause expression, defaults to singleton aspect * &lt;p/&gt; * Valid values are &quot;&quot; (singleton), &quot;perthis(...)&quot;, etc */ public String value() default &quot;&quot;;&#125; 如上代码是@Aspect注解的定义，ElementType.TYPE可以看出该注解作用目标是接口、类、枚举、注解，@Aspect注解，Spring通过@Aspect注解切面并把它应用到目标对象上。 12345678910111213141516171819@Retention(RetentionPolicy.RUNTIME)@Target(ElementType.METHOD)public @interface Pointcut &#123; /** * The pointcut expression * We allow &quot;&quot; as default for abstract pointcut */ String value() default &quot;&quot;; /** * When compiling without debug info, or when interpreting pointcuts at runtime, * the names of any arguments used in the pointcut are not available. * Under these circumstances only, it is necessary to provide the arg names in * the annotation - these MUST duplicate the names used in the annotated method. * Format is a simple comma-separated list. */ String argNames() default &quot;&quot;;&#125; 如上代码是@Pointcut注解的定义，value()用来定义切面所在的位置，定义方式有以下几种方式：123456789// execution定义切面，匹配符合表达式的所有方法@Pointcut(&quot;execution(* com.xjtu.springbootstudy.aop.bymyself.service.ProgrammerService.work())&quot;)// within用于匹配类，对应类下的所有方法都执行切面方法；@Pointcut(&quot;within(com.xjtu.springbootstudy.aop.bymyself.service.*)&quot;)// @annotation用于匹配自定义注解，如下面的@SignLog注解，再将@SignLog放在想定义切面的方法@Pointcut(&quot;@annotation(com.xjtu.springbootstudy.aop.bymyself.annotation.SignLog)&quot;)// @within用于匹配自定义注解，如下面的@SignLog注解，再将@SignLog放在想定义切面的类上@Pointcut(&quot;@within(com.xjtu.springbootstudy.aop.bymyself.annotation.SignLog))&quot;)public void log()&#123; &#125; 如上对log()添加注解，@Pointcut注解中value定义切面位置，使用execution、within、@annotation、@within等方式设置切面。 @before在目标方法开始执行时执行； @after在目标方法执行结束前执行； @AfterReturning在目标方法执行正确返回前执行； @AfterThrowing在目标方法执行异常时执行， @Around环绕执行，一般是前4个无法实现期望功能时，才使用这个注解。 我写了一个简单示意切面程序，如下所示，对应执行结果也展示下下方。有个疑问就是@after和@AfterReturning注解的方法谁先执行？根据执行结果也可以看出@after注解的方法先执行，@AfterReturning注解的方法后执行。 1234567891011121314151617181920212223242526272829303132333435@Before(value = &quot;log()&quot;)public void signIn(JoinPoint joinPoint)&#123; logger.info(&quot;***********the people signs in******&quot;);&#125;@After(value = &quot;log()&quot;)public void leaveWorkPlace(JoinPoint joinPoint)&#123; logger.info(&quot;***********the people leaves workplace******&quot;);&#125;@AfterReturning(value = &quot;log()&quot;)public void signOut(JoinPoint joinPoint)&#123; logger.info(&quot;***********the people signs out successfully******&quot;);&#125;@AfterThrowing(value = &quot;log()&quot;,throwing = &quot;throwable&quot;)public void happenAccidentWhenWorking(JoinPoint joinPoint,Throwable throwable)&#123; logger.info(&quot;***********the people happens accident when working******&quot;); logger.info(&quot;***********&quot;+throwable.getMessage()+&quot;***********&quot;);&#125;@Around(value = &quot;log()&quot;)public void happenAround(ProceedingJoinPoint joinPoint)&#123; try &#123; joinPoint.proceed(joinPoint.getArgs()); &#125; catch (Throwable throwable) &#123; throwable.printStackTrace(); &#125;&#125;执行结果：***********the people signs in****** // @beforeI&apos;m working in workplace! // 这是目标方法的执行结果***********the people leaves workplace****** // @after***********the people signs out successfully****** // @afterReturning]]></content>
      <categories>
        <category>Spring Boot</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Spring</tag>
        <tag>Spring Boot</tag>
        <tag>AOP</tag>
        <tag>面向切面编程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java面向对象编程知识点]]></title>
    <url>%2F2019%2F02%2F20%2FJava%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1%E7%BC%96%E7%A8%8B%E7%9F%A5%E8%AF%86%E7%82%B9%2F</url>
    <content type="text"><![CDATA[面向对象的三个基本特征和五种设计原则三个基本特征封装封装就是把客观事物封装成抽象的类，并且类可以把自己的数据和方法只让可信的类或者对象操作，对不可信的进行信息隐藏； 继承继承是指这样一种能力：它可以使用现有类的所有功能，并在无需重写原有类的情况下对这些功能进行扩展。通过继承创建的新类称为“子类”或“派生类”，被继承的类称为“父类”或“基类”。继承的过程就是从一般到特殊的过程。要实现继承可以通过“继承”和“组合”两种方式实现。在某些OOP语言中，一个子类可以继承多个基类，但一般情况下（Java语言），一个子类只有一个基类，实现多重继承可以通过多级继承来实现。继承概念的实现方式有三类： 实现继承：指使用基类的属性和方法而无需额外编码的能力； 接口继承：指仅使用基类的属性和方法名称，但子类必须提供实现的能力； 可视继承：指子类使用基类的外观和实现代码的能力。多态 多态是允许你将父对象设置成为和一个或多个它的子对象相等的技术，赋值之后，父对象就可以根据当前赋值给它的子对象的特性以不同方式运作。简单地说就是：允许将子类类型的指针赋值给父类类型的指针。实现多态有两种方式：覆盖和重载： 覆盖是指子类重新定义父类的虚函数的做法； 重载是指允许多个同名函数，这些函数的参数不同（参数个数不同，或参数类型不同，或两者都不同）。 五种设计原则单一职责原则(Single Responsibility Principle，SRP)单一职责原则是指一个类的功能要单一，不能包罗万象。一个类应该只有一个引起它变化的原因。 开放封闭原则(Open－Close Principle，OCP) 开放封闭原则指的是对扩展性的开放和对修改的封闭： 对扩展性的开放：模块的行为应该是可扩展的，从而该模块可表现出行的行为以满足需求的变化； 对修改的封闭：模块自身的代码时不应该被修改的，扩展模块的一般途径是修改内部实现。 里氏替换原则(the Liskov Substitution Principle，LSP)子类型必须能够替换掉它们的父类型，并出现在父类能够出现的任何地方。 依赖倒置原则(the Dependency Inversion Principle，PIP)具体依赖抽象，上层依赖下层。 接口隔离原则(the Interface Segregation Principle，ISP)模块间要通过抽象接口隔离开，而不是通过具体的类强耦合起来。 Class.java类文件下getMethods()和getDeclaredMethods()的区别1234567891011@CallerSensitivepublic Method[] getMethods() throws SecurityException &#123; checkMemberAccess(Member.PUBLIC, Reflection.getCallerClass(), true); return copyMethods(privateGetPublicMethods());&#125;@CallerSensitivepublic Method[] getDeclaredMethods() throws SecurityException &#123; checkMemberAccess(Member.DECLARED, Reflection.getCallerClass(), true); return copyMethods(privateGetDeclaredMethods(false));&#125; getMethods()是获取对应类及其所有父类中的共有（public）方法；getDeclaredMethods()获取的是当前类中的所有方法。]]></content>
      <categories>
        <category>工作</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>面向对象</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[StackGAN: Text to Photo-realistic Image Synthesis with Stacked Generative Adversarial Networks]]></title>
    <url>%2F2019%2F02%2F19%2FStackGAN-Text-to-Photo-realistic-Image-Synthesis-with-Stacked-Generative-Adversarial-Networks%2F</url>
    <content type="text"><![CDATA[Abstract源码地址：https://github.com/hanzhanggit/StackGAN 这篇文章提出Stacked Generative Adversarial Networks (StackGAN)合成高分辨率（$256 \times 256$）的逼真图像。这个模型将困难的图像合成问题分成两个子问题：Stage-I GAN根据输入的文本描述合成对象的基本形状和颜色；Stage-II GAN根据Stage-I GAN以合成的图像结果和文本描述作为输入，丰富合成图像的细节，生成逼真的高分辨率图像。 我们都知道生成对抗网络的训练过程存在不稳定性，作者提出了一种新的条件增强技术促进潜在条件流形（latent conditioning manifold）的平滑性，从而改善了合成图像的多样性和训练生成对抗网络过程的稳定性。 Stacked Generative Adversarial Networks Stacked Generative Adversarial Networks的结构如下图所示分为Stage-I GAN和Stage-II GAN两个子网络： Stage-I GAN：以文本描述和噪声向量为输入，文本描述控制对象的基本形状和颜色，噪声控制背景布局，从而生成一张低分辨率图像； Stage-II GAN：纠正Stage-I生成的低分辨率图像的缺陷，再次通过读取文本描述来补全图像的细节，从而生成高分辨率的逼真图像。 Stage-I GANKL散度（Kullback–Leibler divergence,KL divergence）本文章中的条件增强技术中计算标准高斯分布和条件高斯分布之间的KL散度，所以先简单介绍KL散度。KL散度又被称为相对熵（relative entropy）或者信息散度，是连个概率分布间差异的非对称性度量。 $$KL(p(x)||q(x))=\sum_x{p(x)\log{\frac{p(x)}{q(x)}}}\\=\sum_x{p(x)\log{p(x)}-p(x)\log{q(x)}}$$ KL散度是衡量两个分布之间的差异大小的，KL散度大于等于0，并且越接近0说明p与q这两个分布越像，当且仅当p与q相等时KL散度取0。 模型框架对于生成器$G_0$，为了获得文本条件变量$\hat{c}_0$，文本嵌入$\varphi_t$首先通过全连接层生成$\mu_0$和$\delta_0$（$\delta_0$是$\sum_0$的对角线元素值）。$\hat{c}_0$由高斯分布$N(\mu(\varphi(t)),\sum_0(\varphi(t)))$抽样得到。$N_g$维的条件向量$\hat{c}_0$由公式$\hat{c}_0=\mu_0+\delta_0 \bigodot\epsilon$计算得到，其中$\bigodot$表示元素乘积（element-wise multiplication），$\epsilon\sim\N(0,I)$。之后，$\hat{c}_0$与$N_z$维的噪声向量拼接并通过一系列的上采样块生成尺寸为$W_0\times H_0$的图像。对于判别器$D_0$,文本嵌入$\varphi(t)$通过全连接压缩为$N_d$维然后通过复制将其转换成$M_d \times M_d \times N_d$大小的张量。同时图像经过一系列下采样块，输出$M_d \times M_d$大小的张量。最后把图像和文本得到的张量拼接在一起，经过$1\times 1$的卷积层以及一个只有一个节点的全连接层，从而得到决策值。Stage-I训练过程损失函数如下，目的在于最大化$L_{D_0}$以训练判别器，最小化$L_{G_0}$以训练生成器。 $$L_{D_0}=E_{(I_0,t)\sim{p_{data}}}[\log D_0(I_0,\varphi_t)]+E_{z\sim{p_z},t\sim p_{data}}[\log {(1-D_0(G_0(z,\hat{c}_0),\varphi_t)}]\\L_{G_0}=E_{z\sim{p_z},t\sim p_{data}}[\log {(1-D_0(G_0(z,\hat{c}_0),\varphi_t)}]+\lambda{D_{KL}}(N(\mu_0(\varphi_t),\sum_0(\varphi_t)||N(0,I)),$$ Stage-II GANStage-II GAN的模型框架和Stage-I GAN相似，只不过没有了噪声输入，换成了Stage-I合成的低分率图像。Stage-II其损失函数如下，目的在于最大化$L_D$以训练判别器，最小化$L_G$以训练生成器。 $$L_D=E_{(I,t)\sim{p_{data}}}[\log D(I,\varphi_t)]+E_{s_0\sim{p_{G_0}},t\sim p_{data}}[\log {(1-D(G(s_0,\hat{c}),\varphi_t)}]\\L_G=E_{s_0\sim{p_{G_0}},t\sim p_{data}}[\log {(1-D(G(s_0,\hat{c}),\varphi_t)}]+\lambda{D_{KL}}(N(\mu(\varphi_t),\sum(\varphi_t)||N(0,I)),$$]]></content>
      <categories>
        <category>科研</category>
      </categories>
      <tags>
        <tag>论文阅读</tag>
        <tag>text2image</tag>
        <tag>图像合成</tag>
        <tag>生成对抗网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java内存模型（二）]]></title>
    <url>%2F2019%2F02%2F19%2FJava%E5%86%85%E5%AD%98%E6%A8%A1%E5%9E%8B%EF%BC%88%E4%BA%8C%EF%BC%89%2F</url>
    <content type="text"><![CDATA[Java内存模型重排序重排序是指编译器和处理器为了优化程序性能而对指令序列进行重新排序的手段。 数据依赖性如果两个操作访问同一个变量，且这两个操作中有一为写操作，此时这两个操作时间就存在数据依赖性。数据依赖分为三种类型：写后读、写后写、读后写。上述三种情况，只要重排序这两个操作的执行顺序，程序的执行结果就会改变。因此编译器和处理器在重排序时，会遵守数据依赖性，编译器和处理器不会改变存在数据依赖关系的两个操作顺序。这类所说的数据依赖性仅针对单个处理器中执行的指令序列和单个线程中执行的操作，不同处理器之间和不同线程之间的数据依赖性不被编译器和处理器考虑。 as-if-serial语义as-if-serial语义的意思是：不管怎么重排序（编译器和处理器为了提高并行度），（单线程）程序的执行结果不会被改变。as-if-serial语义把单线程程序保护起来，遵循as-if-serial语义的编译器、runtime和处理器保证单线程程序执行的结果与其按照顺序执行的结果一致。 程序顺序规则123double pi = 3.14; //Adouble r = 1.0; //Bdouble area = pi * r * r; //C 根据happens-before的程序顺序规则，上面计算圆面积的示例代码存在3个happens-before关系： A happens-before B B happens-before C A happens-before C 这里A happens-before B，但在实际执行时B却可以在A之前执行。如果A happens-before B，JMM并不一定要求A在B之前执行。JMM仅仅要求前一个操作对后一个操作可见，且前一个操作按顺序排在后一操作之前。这里操作A的执行结果并不需要对操作B可见；而且重排序操作A和操作B后的执行结果，与操作A和操作B按照A happens-before B顺序执行的结果一致。在这种情况下，JMM会认为这种重排序不非法（not illegal）,JMM允许这种重排序。在计算机中，软件技术和硬件技术有一个共同目的：在不改变程序执行结果的前提下，尽可能提高并行度。 重排序对多线程的影响多线程程序中，对存在控制依赖的操作重排序，可能会改变程序的执行结果。 顺序一致性数据竞争与顺序一致性当程序未正确同步时，就可能会存在数据竞争。Java内存模型规范对数据竞争的定义如下： 在一个线程中写入一个变量； 在另一个线程中读同一个变量； 而且写和读没有通过同步来排序。 当代码中包含数据竞争时，程序的执行往往产生违反直觉的结果。如果一个多线程程序能够同步，这个程序将是一个没有数据竞争的程序。JMM对正确同步的多线程程序的内存一致性做了如下保证：如果程序时正确同步的，程序的执行将具有顺序一致性——即程序的执行结果与改程序在顺序一致性内存模型中的执行结果相同。 顺序一致性内存模型顺序一致性内存模型是一个被计算机科学家理想化的理论参考模型。它为程序员提供了极强的内存可见性保证。顺序一致性内存模型有两大特性： 一个线程中的所有操作都必须按照程序的顺序来执行； （不管线程是否同步）所有线程都只能看到一个单一的操作执行顺序。在顺序一致性内存模型中，每个操作都必须原子执行且立刻对所有线程可见。]]></content>
      <categories>
        <category>Java并发编程</category>
      </categories>
      <tags>
        <tag>重排序</tag>
        <tag>数据依赖性</tag>
        <tag>顺序一致性</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java内存模型（一）]]></title>
    <url>%2F2019%2F02%2F18%2FJava%E5%86%85%E5%AD%98%E6%A8%A1%E5%9E%8B%EF%BC%88%E4%B8%80%EF%BC%89%2F</url>
    <content type="text"><![CDATA[Java内存模型Java内存模型的基础并发编程模型的两个关键问题 线程之间如何通信？ 线程之间如何同步？ 通信是指线程之间以何种机制交换信息，在命令式编程中线程之间的通信机制有两种：共享内存和消息传递。在共享内存模型中，线程之间共享程序的公共状态，通过读写内存中的公共状态进行隐式通信；而在消息传递的并发模型中，线程之间没有公共状态，线程之间必须通过发送消息来显式进行通信。同步是指程序中用来控制不同线程间操作发生的相对顺序的机制。在共享内存并发模型中，同步是显式进行的。程序员必须显式指定某个方法或某个代码段需要在线程之间互斥执行。在消息传递的并发模型中，由于消息的发送必须在消息的接收之前，因此同步是隐式进行的。Java的并发采用的是共享内存模型，Java线程之间的通信总是隐式进行的，整个通信过程对于程序员完全透明。 Java内存模型的抽象结构在Java中，所有实例域、静态域和数组元素都存储在堆内存中，堆内存在线程之间共享。局部变量（Local Variables），方法定义参数（Formal Method Parameters）和异常处理器参数（Exception Handler Parameters）不会在线程之间共享，它们不会有内存可见性问题，也不受内存模型的影响。Java线程之间的通信由Java内存模型（JMM）控制，JMM决定一个线程对共享变量的写入何时对另一个线程可见。从抽象的角度来看，JMM定义了线程和主内存之间的抽象关系：线程之间的共享变量存储在主内存（Main Memory）中，每个线程都有一个私有的本地内存（Local Memory），本地内存中存储了该线程读写共享变量的副本。本地内存是JMM的一个抽象概念，它涵盖了缓存、写缓冲区、寄存器以及其他的硬件和编译器优化。 由上图看出线程A和线程B之间要通信的话，必须经历以下两个步骤：1）.线程A把本地内存A中更新过的共享变量 刷新到主内存中去；2）.线程B到主内存中去读取线程A之前已更新过的共享变量。 指令重排序在执行程序时，为了提高性能，编译器和处理器常常会对指令做重排序。重排序分为3种类型。1）编译器优化的重排序。编译器再不改变单线程程序语义的前提下，可以重新安排语句的执行顺序。2）指令级并行的重排序。现代处理器采用了指令级并行技术（Instruction-Level Parallelism，ILP）来将多条指令重叠执行。如果不存在数据依赖性，处理器可以改变语句对应机器指令的执行顺序。3）内存系统的重排序。由于处理器使用缓存和读写缓冲区，这使得加载和存储操作看上去可能是乱序执行。 happens-before在JMM中，如果一个操作执行的结果需要对另一个操作可见，那么这两个操作之间必须要存在happens-before关系。这里提到的两个操作，既可以在一个线程之内，也可以在不同线程之间。与程序员密切相关的happens-before规则如下： 程序顺序规则：一个线程中的每个操作，happens-before于该线程中的任意后续操作。（也就是说一个线程中的每个操作的结果对于该线程中的其他操作都是可见的） 监视器锁规则：对一个锁的解锁，happens-before于随后对这个锁的加锁。（也就是说一个锁的解锁操作需要对该锁的解锁操作可见） volatile变量规则：对一个volatile域的写，happens-before于任意后续对这个volatile域的读。 传递性：如果A happens-before B，且B happens-before C，那么A happens-before C。]]></content>
      <categories>
        <category>Java并发编程</category>
      </categories>
      <tags>
        <tag>Java内存模型</tag>
        <tag>共享内存</tag>
        <tag>happens-before</tag>
        <tag>指令重排序</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[并发机制的底层实现原理]]></title>
    <url>%2F2019%2F01%2F21%2F%E5%B9%B6%E5%8F%91%E6%9C%BA%E5%88%B6%E7%9A%84%E5%BA%95%E5%B1%82%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86%2F</url>
    <content type="text"><![CDATA[并发机制的底层实现原理volatile在多线程并发编程中synchronized和volatile关键字都扮演着重要角色，volatile是轻量级的synchronized，它在多处理器开发中保证了共享变量的“可见性”。可见性的意思是当一个线程修改一个共享变量时，另外的线程可以读到修改后的值。volatile的恰当使用能比synchronized关键字的使用和执行成本更低，因为它不会引起线程上下文的切换和调度。定义：Java编程语言允许线程访问共享变量，为了确保共享变量能被准确和一致地更新，线程应该确保通过排他锁单独获得这个变量。Java语言提供了volatile，在某些情况下比锁要更加方便。如果一个字段被声明为volatile，Java线程内存模型确保所有线程看到这个变量的值是一致的。在X86处理器下通过工具获取JIT编译器生成的汇编指令，查看对volatile变量进行写操作时，对应CPU的操作，例子如下： Java代码如下： 1instance = new Singleton(); //instance是volatile变量 转化为汇编代码，如下： 10x01a3de1d: movb $0x0,ox1104800(%esi);0x01a3de24: lock addl $0x0,(%esp); 有volatile变量修饰的共享变量进行写操作时会多出两行汇编代码，通过查看IA-32架构软件开发者手册可知，Lock前缀的指令在多核处理器中会发生以下两件事情。 将当前处理器的缓存行的数据写回到系统内存； 这个写回内存的操作会是其他CPU里缓存了该内存地址的数据无效。 synchronized在多线程并发编程中synchronized一直是元老级角色，很多人称呼它为重量级锁。Java SE1.6对synchronized进行了各种优化，为减少获得锁和释放锁带来的性能消耗而引入的偏向锁和轻量级锁。synchronized实现同步的基础：Java中的每一个对象都可以作为锁，具体表现为一些3种形式： 对于普通同步方法，锁是当前实例对象； 对于静态同步方法，锁是当前类的Class对象； 对于同步代码块，锁是Synchronized括号里配置的对象。 当一个线程师徒访问同步代码块时，它首先必须得到锁，退出或者抛出异常时必须释放锁。从JVM规范中可以看到Synchronized在JVM里实现的原理，JVM基于进入和退出Monitor对象来实现方法同步和代码块同步，但两者实现细节不同。代码块同步用monitorenter和monitorexit指令实现，而方法同步是使用另一种方式实现的，细节没有在JVM规范中提及。但方法的同步同样可以使用上述两个指令实现。monitorenter指令是在编译后插入到同步代码块的开始位置，而monitorexit是插入到方法结束处和异常处，JVM要保证每个monitorenter必须有对应的monitorexit与之配对。任何对象都有一个monitor与之关联，当且一个monitor被持有之后，它将处于被锁状态。线程执行到monitor指令时，将尝试获取对象所对应的monitor的所有权，即尝试获得对象的锁。 锁的升级Java SE 1.6为了减少获得锁和释放锁带来的性能消耗，引入了偏向锁和轻量级锁，在Java SE 1.6中，锁一共有4个状态，级别从低到高依次是：无锁状态、偏向锁状态、轻量级锁状态和重量级锁状态，这几个状态随着竞争情况逐渐升级。锁可以升级但不可以降级，意味着偏向锁升级为轻量级锁后不能降级为偏向锁。这种锁只能升级不能降级的策略是为了提高获得锁和释放锁的效率。 原子操作原子操作（atomic operation）指的是不可被中断的一个或者一系列操作。处理器保证从系统内存中读取或写入一个字节是原子的，意思是当一个处理器读取一个字节时，其他处理器不能访问这个字节的内存地址。但是负责的内存操作处理器是不能自动保证原子性的，比如跨总线宽度、跨多个缓存行和跨页表的访问。但是，处理器提供总线锁定和缓存锁定两个机制来保证复杂内存操作的原子性。 总线锁是指使用处理器提供的一个LOCK#信号，当一个处理器在总线上输出此信号时，其他处理器的请求将被阻塞住，那么该处理器就可以独占共享内存。但是总线锁定把CPU和内存之间的通信锁定了，这使得锁定期间，其他处理器不能操作其他内存地址的数据，所以总线锁定开销比较大。 缓存锁定是指内存区域如果被缓存在处理器的缓存行中，并且在LOCK操作期间被锁定，那么当它执行写操作回写到内存时，处理器不在总线上声言LOCK#信号，而是修改内部的内存地址，并允许它的缓存一致性机制来保证操作的原子性，因为缓存一致性机制会阻止同时修改由两个以上处理器缓存的内存区域数据，当其他处理器回写已被锁定的缓存行的数据时，会使缓存行无效 CAS实现原子操作的三大问题： ABA问题； 循环时间长开销大； 只能保证一个共享变量的原子操作。]]></content>
      <categories>
        <category>Java并发编程</category>
      </categories>
      <tags>
        <tag>synchronized</tag>
        <tag>volatile</tag>
        <tag>原子操作</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[并发编程的挑战]]></title>
    <url>%2F2019%2F01%2F20%2F%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E7%9A%84%E6%8C%91%E6%88%98%2F</url>
    <content type="text"><![CDATA[并发编程的挑战上下文切换在多线程执行代码的过程中，CPU为每个线程分配CPU时间片保证线程的执行，时间片很短，一般是几十毫秒（ms）。CPU通过时间片分配算法来循环执行任务，当前任务执行一个时间片后会切换到下一个任务。但是，在切换前会保存前一个任务的状态，以便下次切换回这个任务的时候可以再加载这个任务。任务从保存到再加载的过程就是一次上下文切换。多线程由于上下文切换的存在使得其不一定比单线程快，一般减少上下文切换的方法有无锁并发编程、CAS算法、使用最少线程和使用协程。 无锁并发编程：多线程竞争锁时，会引起上下文切换，所以多线程处理数据时，可以用一些方法来避免使用锁，如将数据的ID按照Hash算法取模分段，不同线程处理不同段的数据； CAS算法：Java的Atomic包使用CAS算法来更新数据，而不需要加锁； 使用最少线程：避免创建不需要的线程，比如任务少时，创建很多线程会造成大量线程都处于等待状态； 使用协程：在单线程中实现多任务的调度，并在单线程里维持多个任务间的切换。死锁死锁是指由于两个或者多个任务互相持有对方所需要的资源，导致这些线程处于等待状态，无法前往执行。 线程死锁产生的四个必要条件 互斥条件：某种资源一次只允许一个线程访问，即该资源一旦分配给某个线程，其他线程就不能再访问，直到该进程访问结束并释放； 请求和保持条件：指线程已经保持至少一个资源，但又提出了新的资源请求，而该资源已被其它线程占有，此时请求进程阻塞，但又对自己已获得的其它资源保持不放； 不剥夺条件：指线程已获得的资源，在未使用完之前，不能被剥夺，只能在使用完时由自己释放； 环路(循环)等待条件：指在发生死锁时，必然存在一个线程——资源的环形链，即线程集合{t0，t1，t2，···，tn}中的t0正在等待一个t1占用的资源；t1正在等待t2占用的资源，……，tn正在等待已被t0占用的资源。 避免死锁的几个常用方法 避免一个线程同时获得多个锁； 避免一个线程在锁内同时占用多个资源，尽量保证每个锁只占用一个资源； 尝试使用定时锁，使用lock。tryLock(timeout)来替代使用内部锁机制； 对于数据库锁，加锁和解锁必须在一个数据库连接里，否则会出现解锁失败的情况。 资源限制的挑战资源限制是指在进行并发编程时，程序的执行速度受限于计算机硬件资源或软件资源。例如，网络带宽的限制、硬盘读写速度的限制、CPU计算速度的限制、数据库的连接数和socket连接数限制等。 资源限制引发的问题在并发编程中，将代码执行速度加快的原则是将代码中串行执行的部分变为并发执行，但如果将某段串行代码并发执行，因为资源受限，仍然在串行执行，这时候程序不仅不会加快执行，反而会更慢，因为增加了上下文切换和资源调度的时间。 资源限制的解决对应硬件资源限制，可以考虑使用集群并行执行程序。既然单机的资源有限制，那么就让程序在多机上运行。比如用ODPS、Hadoop或者自己搭建服务器集群，不同的机器处理不同的数据。对于软件资源限制，可以考虑使用资源池复用资源。比如使用连接池将数据库和Socket连接复用，或者在调用对方webservice接口获取数据时，只建立一个连接。 资源限制下的并发编程根据不同的资源限制调整程序的并发度，比如下载文件程序依赖于两个资源——带宽和硬盘读写速度。有数据库操作时，涉及数据库连接数，如果SQL语句执行非常快，而线程的数量比数据库连接数大很多，就会导致某些线程被阻塞，等待数据库连接，因此这个时候就应该降低线程数量。]]></content>
      <categories>
        <category>Java并发编程</category>
      </categories>
      <tags>
        <tag>上下文切换</tag>
        <tag>死锁</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kafka-分布式流平台]]></title>
    <url>%2F2019%2F01%2F16%2FKafka-%E5%88%86%E5%B8%83%E5%BC%8F%E6%B5%81%E5%B9%B3%E5%8F%B0%2F</url>
    <content type="text"><![CDATA[Kafka:分布式流平台参考链接：Kafka官网 Apache Kafka是一个分布式流平台，具有以下三个关键能力： 发布和订阅记录流（streams of records），类似于一个消息队列或者是企业消息系统； 以具有容错性和持久性的方式存储记录流； 即时处理记录流 Kafka通常被用在两大类应用中： 构建可在系统或应用程序之间可靠获取数据的实时流数据管道 构建转换或响应数据流的实时流应用 一些概念： Kafka作为一个集群可以运行在一个或多个服务器上，这些服务器可跨多个数据中心； Kafka集群存储在类别中的记录流称为主题（topics）； 每个记录包含一个键（key）、一个值（value）以及一个时间戳（timestamp）； Kafka有四个核心API： 生产者（Producer）API允许一个应用将记录流发布到一个或多个Kafka主题（topics）上； 消费者（Consumer）API允许一个应用去订阅一个或者多个主题（topics），并处理这些给它们生产的记录流； 流（Streams）API允许一个应用扮演流处理器的角色，从一个或多个主题消费一个输入流，然后向一个或多个主题输出流，高效地进行输入流到输出流的转换； 连接（Connector）API允许创建或者运行可重用的生产者或消费者，并与将Kafka主题与现有的应用或数据系统连接。例如关系数据库的连接器可以捕获对表的每个更改。 主题（Topics）和记录（Logs）一个主题是发布记录的一个类别或订阅源名称。 Kafka中主题总是多订阅用户; 也就是说，一个主题可以有零个，一个或多个消费者订阅写入它的数据。对于每个主题，Kafka群集都维护一个如下所示的分区日志（partitioned log）： 每个分区（partition）都是有序的，不可变的记录序列，这些记录不断添加到结构化的提交日志中。分区中的每个记录都被分配一个称为偏移的序列id，这些id唯一标识了分区中的每个记录。Kafka集群使用可配置的保留期，持久保存所有已发布的记录，无论这些记录是否已被使用。 例如，如果保留策略被设置为两天，那么在发布记录后的两天内，记录都可以被消费，之后将被丢弃以释放空间。 Kafka的性能不受数据大小的影响，因此长时间存储数据不是问题。实际上，基于每个消费者保留的唯一元数据是该消费者在日志中的偏移或位置。 这种偏移由消费者控制：通常消费者会线性地提高其偏移量从而读取记录，但事实上，由于该位置由消费者控制，因此它可以按照自己喜欢的任何顺序去消费记录。 例如，消费者可以重置为较旧的偏移量去重新处理过去的数据，或者跳到最近的记录并从“现在”开始消费。 这些特性的组合意味着Kafka消费者非常方便，消费者的加入移除对集群或其他消费者没有太大影响。 例如，您可以使用我们的命令行工具“拖尾（tail，此处不太理解）”任何主题的内容，而无需更改任何现有使用者所消耗的内容。 日志中的分区有多种用途。 首先，它们允许日志扩展到超出适合单个服务器的规模。 每个独立分区必须适合托管它的服务器，但一个主题可能有多个分区，因此它可以处理任意数量的数据。 其次，分区充当了并行性的单元。 一些基本概念 Consumer Group：逻辑概念，对于同一个topic，会广播给不同的group。同一个group中只有一个consumer可以消费该topic。 Broker：物理概念，Kafka集群中的每个Kafka节点。 Partition：物理概念，Kafka下数据存储的基本单元。一个Topic数据，会被分散存储到多个Partition，每个Partition是有序的。 Replication：同一个Partition可能会有多个副本（replication），多个副本之间数据是一致的。 Replication Leader：一个Partition的多个副本需要选举一个leader负责partition上与Producer和Consumer消息传递。 ReplicaManager：负责管理当前Broker所有分区和副本的信息，处理KafkaController发起的请求，如副本状态的切换（重新选举Replication Leader）、添加消息、消费消息等。]]></content>
      <categories>
        <category>kafka</category>
      </categories>
      <tags>
        <tag>kafka</tag>
        <tag>开源软件</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[图片服务器搭建]]></title>
    <url>%2F2019%2F01%2F14%2F%E5%9B%BE%E7%89%87%E6%9C%8D%E5%8A%A1%E5%99%A8%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[图片服务器搭建环境：window server 2016 Datacenter 首先下载nginx软件包，我下在的版本是nginx-1.15.8，下载链接和英文安装教程如下: nginx下载地址 nginx英文安装教程 解压nginx-1.15.8.zip，目录结构如下图所示： 运行nginx.exe,并查看通过命令查看运行状态，如下所示： 如果没有运行成功，可以通过打开logs\error.log查看错误记录。 打开浏览器，输入localhost测试是否成功，如下图所示： 搭建图片服务器 首先打开conf目录下的nginx.conf配置文件，修改如下： 重新启动nginx，一些命令如下： 1234nginx -s stop fast shutdownnginx -s quit graceful shutdownnginx -s reload changing configuration, starting new worker processes with a new configuration, graceful shutdown of old worker processesnginx -s reopen re-opening log files 此时在images文件夹下放一张随意图片（我放了一张nginx网站的截图），打开浏览器，输入链接http://47.95.145.72:8090/1.png，就可以看到我们的图片，如下所示： 到这里，图片服务器就搭建完成了！]]></content>
      <categories>
        <category>工程</category>
      </categories>
      <tags>
        <tag>nginx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[垃圾收集（二）]]></title>
    <url>%2F2019%2F01%2F14%2F%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%EF%BC%88%E4%BA%8C%EF%BC%89%2F</url>
    <content type="text"><![CDATA[垃圾收集垃圾收集算法标记-清除算法算法分为标记阶段和清除阶段 ，首先标记出所有需要回收的对象，在标记完成后统一回收所有被标记的对象，它的标记过程就是引用计数法或者可达性算法判断对象是否存活的过程。不足： 效率问题，标记和清除两个阶段的效率都不高； 空间问题，标记清除之后会产生大量不连续的内存碎片，空间碎片太多可能会导致以后程序运行过程中需要分配较大对象时，无法找到足够的内存碎片而不得不提前出发一次垃圾收集动作。标记清除算法的执行过程如下图所示： 复制算法为了解决效率问题，一种“复制”的垃圾收集算法出现了，它将可用的内存分成大小相等的两个区域，每次只使用其中的一块区域。当这块区域用完之后，就将还存活着的对象复制到另一块内存区域上，然后再把已使用过的内存空间一次清理掉。这样使得每次都是对整个半区进行垃圾回收，内存分配时也就不用再考虑内存碎片等复杂情况，只要移动堆顶指针，按顺序分配内存即可，实现简单，运行高效。只是这种算法每次只能使用一半的内存，相当于将内存缩小为原来的一半，代价太高。复制算法的执行过程如下图所示： 目前的商业虚拟机都是采用复制算法回收新生代，但是并不按照1:1划分内存空间，而是将内存分为一块较大的Eden空间和两块较小的Survivor空间，每次使用Eden和其中一块Survivor。当回收时，将Eden和Survivor中还存活的对象一次性复制到另一块Survivor空间上，最后清理掉Eden和刚才用过的Survivor空间。HotSpot虚拟机默认Eden：Survivor=8:1，也就是每次新生代中可用内存空间为整个新生代容量的90%，只有10%的内存被“浪费”。当然在大多数（98%）情况下，只有不超过10%的对象存活，而当超过10%时，也就意味着Survivor空间不够用，这个时候就需要依赖老年代进行分配担保（无法再Survivor区存放的对象直接进入老年代）。 标记-整理算法复制算法在对象存活率较高的情况下就会进行较多的复制操作，效率将会变低，而且还需要额外的内存空间进行分配担保，以应对内存（Survivor空间）不够容纳存活对象的极端情况。因此，老年代一般不采用复制算法，而采用“标记-整理”（Mark-Compact）算法，标记过程仍然与“标记-清除”算法一样，但后续步骤不是直接对可回收对象进行清理，而是让存活对象都向一端移动，然后直接清理掉端边界以外的内存，标记-整理算法的示意图如下： 分代收集算法当前商业虚拟机的垃圾收集都采用“分代收集”（Generational Collection）算法，这种算法根据对象存活周期的不同将内存划分为几块。一般是把Java堆划分成新生代和老生代，从而根据各个年代的特点采用最适当的收集算法。在新生代中，每次垃圾收集都会发现大量的对象死去，只有少量对象存活，因此采用复制算法，只需要付出少量存活对象的复制成本就可以完成收集；在老年代中因为对象存活率高、没有额外的空间对它进行分配担保，就必须采用“标记-清理”或者“标记-整理”算法进行回收。 HotSpot的算法实现枚举根节点从可达性分析中从GC Roots节点找到引用链这个操作为例，可作为GC Roots的加点主要在全局性的引用（例如常量或类静态变量）与执行上下文（例如栈帧中的本地变量表）中，现在的很多应用仅仅方法区就有数百兆，如果要逐个检查这里面的引用，那么必然会消耗很多时间。另外，可达性分析对执行时间的敏感还体现在GC停顿上，因为这项分析工作必须在一个能确保一致性的快照中进行——这里的“一致性”的意思是在真个分析过程中整个执行系统看起来就像被冻结在某个时间点上，不可以出现分析过程中对象引用关系还在不断变化的情况，该点不满足的话分析结果准确性就无法得到保证。这点是导致在GC进行时必须停止多有Java执行线程的其中一个重要原因。即时在号称（几乎）不会发生停顿的CMS收集器中，枚举根节点时也是必须要停顿的。由于目前主流的Java虚拟机使用的都是准确式GC，所以当执行系统停顿下来后，并不需要一个不漏地检查完所有执行上下文和全局的引用位置，虚拟机应当是有办法直接得知哪些地方存放着对象引用。在HotSpot的实现中，是使用一组称为OopMap的数据结构来达到这个目的，在类加载完成时，HotSpot就把对象内什么偏移量上是什么类型的数据计算出来，在JIT编译过程中，也会在特定位置记录下栈和寄存器哪些位置是引用。 安全点在OopMap的协助下，HotSpot可以快速且准确地完成GC Roots枚举，但一个很现实的问题随之而来：可能导致引用关系发生变化，或者说OopMap内容变化的指令非常多，如果每一条指令都生成对应的OopMap，那将会需要大量的额外空间，这样GC的空间成本就会变得很高。实际上，HotSpot也的确没有为每条指令都生成OopMap，前面已经提到只是在“特定的位置”记录这些信息，这些位置称为安全点（Safepoint），即程序执行时并非在所有的地方都能停顿下来开始GC，只有到达安全点时才能暂停。Safepoint的选定既不能太少以致于GC等待时间过长，也不能过于频繁以致于过分增大运行时的负荷。所以，安全点的选定基本上是以程序“是否具有让程序长时间执行的特征”为标准进行选定的————因为每一条指令执行的时间都非常短暂，程序不太可能因为指令流长度太长这个原因而过长时间运行，“长时间执行”最明显的特征就是指令序列复用，例如方法调用、循环跳转、异常跳转等，所以具有这些功能的指令才回产生Safepoint。对于Safepoint，另一个需要考虑的问题就是如何在GC发生时让所有的线程都“跑”到最近的安全点上停顿下来。这里有两种方案可以选择:抢先式中断（Preemptive Suspension）和主动式中断（Voluntary Suspension），其中抢先式中断不需要线程的执行代码主动去配合，在GC发生时，首先把所有的线程全部中断，如果发现线程中断的地方不在安全点上，就恢复线程，让它“跑”到安全点上。现在几乎没有虚拟机实现采用抢先式中断来暂停线程从而相应GC时间。而主动式中断的思想是当GC需要中断线程的时候，不直接对线程操作，仅仅简单地设一个标志，各个线程执行时主动去轮询这个标志，发现中断标志为真时就自己中断挂起。 安全区域使用Safepoint似乎已经完美解决了如何进入GC的问题，但实际情况却并不一定。Safepoint机制保证了程序执行时，在不太长的时间内就会遇到可进入GC的Safepoint。但是程序不执行的时候，CPU没有分配时间给程序，典型例子就是线程处于Sleep或者Blocked状态，这时线程无法响应JVM的中断请求，“走”到安全的地方去挂起中断，JVM也不太可能等待这些线程重新分配CPU时间。对于这种情况就需要安全区域（Safe Region）来解决。安全区域是指在一段代码片段中，引用关系不会发生变化。在这个区域中的任何地方开始GC都是安全的。在线程执行到Saferegion中的代码时，首先标志自己进入了Saferegion，那样当在这段时间里JVM要发起GC时，就不用管标志自己为Saferegion状态的线程了。在线程要离开Saferegion时，它要检查系统是否已经完成了根节点枚举（或者整个GC过程），如果完成了那么线程继续执行，否则它就要等待直到收到可以安全离开Saferegion的信号为止。]]></content>
      <categories>
        <category>深入理解Java虚拟机</category>
      </categories>
      <tags>
        <tag>垃圾收集器</tag>
        <tag>垃圾收集算法</tag>
        <tag>安全点</tag>
        <tag>安全区域</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[垃圾收集(一)]]></title>
    <url>%2F2019%2F01%2F12%2F%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%2F</url>
    <content type="text"><![CDATA[垃圾收集垃圾回收需要关注的事情： 哪些内存需要回收？ Java堆和方法区 什么时候回收？ 如何回收？ java内存运行时各个区域，其中程序计数器、java虚拟机栈、本地方法栈3个区域随线程而生，随线程而灭；栈中的栈帧随着方法的进入和退出而有条不紊地进行出栈和入栈操作。而Java堆和方法区则不一样，一个接口中的多个实现类需要的内存可能不一样，一个方法中的多个分支需要的内存也可能不一样，我们只有在程序处于运行期间时才能知道会创建哪些对象，这部分内存的分配和回收都是动态的，垃圾回收器所关注的就是这部分内存。 对象存活判断引用计数法（Reference Counting）算法描述：给对象中添加一个引用计数器，每当有一个引用指向这个对象，计数器值就加1；当引用失效时，计数器值就减1；任何时刻计数器值为0的对象就不可能再被使用。引用计数法存在一个问题：它很难解决对象之间的相互循环引用问题，举个例子，示例代码如下。对象123456789101112131415161718192021222324252627```/** * 对象objA和objB存在相互引用 * @author yangkuan */public class ReferenceCounteringGC &#123; public Object instance = null; private static final int _1MB = 1024 * 1024; /** * 这个成员变量的意义是通过其占用的内存，通过GC日志查看对象是否被回收 */ private byte[] bigSize = new byte[2 * _1MB]; public static void testGC()&#123; ReferenceCounteringGC objA = new ReferenceCounteringGC(); ReferenceCounteringGC objB = new ReferenceCounteringGC(); objA.instance = objB; objB.instance = objA; objA = null; objB = null; // 假设此处发生垃圾回收，如果回收算法是引用计数法，那么objA和objB将不会被回收 System.gc(); &#125;&#125; 可达性分析算法（Reachability Analysis）算法描述这个算法的基本思想就是通过一系列的称为“GC Roots”的对象作为起始点，从这些节点开始向下搜索，搜索所走过的路径称为引用链（Reference Chain），当一个对象到GC Roots没有任何引用链相连（即该对象到GC Roots不可达）时，则证明这个对象不可用。那这些不可达的对象就可以判定为可回收对象。可作为GC Roots的对象包括以下几种： 虚拟机栈（栈帧中的本地变量表）中引用的对象； 方法区中类静态属性引用的对象； 方法区中常量引用的对象； 本地方法栈中JNI（即Native方法）引用的对象。 引用类型 强引用（Strong Reference）强引用就是指在程序代码之中普遍存在的，类似Object obj = new Object()这类的引用，只要强引用还存在，垃圾收集器永远不会回收掉被引用的对象。 软引用（Soft Reference）软引用是用来描述一些还有用但并非必须的对象。对于软引用关联着的对象，在系统将要发生内存溢出异常之前，将会把这些对象列进回收范围之中进行第二次回收。如果这次回收之后还没有足够的内存，那么程序就会抛出内存溢出异常。 弱引用（Weak Reference）弱引用也是用来描述非必须对象的，但是它的强度比软引用还要更弱一些，被弱引用关联的对象只能生存到下一次垃圾回收之前。当垃圾收集器工作时，无论当内存是否足够，都会回收掉只被弱引用关联的对象。 虚引用（Phantom Reference）虚引用也被称为幽灵引用或者幻影引用，它是最弱的一种引用关系。一个对象是否有虚引用的存在，完全不会对其生存时间构成威胁，也无法通过虚引用来取得一个对象实例。为一个对象设置虚引用关联的唯一目的就是能在这个对象被收集器回收时收到一个系统通知。 对象的两次标记过程 如果对象再进行可达性分析的时候发现其与GC Roots之间不可达，那么它将会被第一次标记并进行下一次筛选，筛选的条件是此对象是否有必要执行finalize()方法。当对象没有覆盖finalize()方法或者finalize()方法已经被虚拟机执行过，都会被认为没有必要执行。 如果这个对象被虚拟机认为有必要执行finalize()方法，那么这个对象将会放置在一个叫F-Queue的队列中，并在稍后由一个由虚拟机自动建立的、低优先级的Finalizer线程去执行它。如果对象想要拯救自己，那么覆盖finalize()方法在方法中重新将自身与引用链上的任意一个对象关联起来就可以避免自己被回收。 回收方法区方法区（永久代）的垃圾回收主要包括两个部分：废弃常量和无用的类。 废弃常量指的是没有任何地方引用这个常量，这个常量会被系统清理出常量池； 无用的类需要同时满足以下三个条件： 该类的所有实例都已经被回收，也就是Java堆中不存在该类的任何实例； 加载该类的ClassLoader已经被回收； 该类对应的java.lang.Class对象没有在任何地方被引用，无法再任何地方通过反射访问该类的方法。]]></content>
      <categories>
        <category>深入理解Java虚拟机</category>
      </categories>
      <tags>
        <tag>内存分配策略</tag>
        <tag>引用计数法</tag>
        <tag>可达性算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java内存区域]]></title>
    <url>%2F2019%2F01%2F10%2FJava%E5%86%85%E5%AD%98%E5%8C%BA%E5%9F%9F%2F</url>
    <content type="text"><![CDATA[Java内存区域Java虚拟机在执行Java程序的过程中，把其所管理的内存划分成多个区域，如下图所示。每个数据区域都有各自的用途，有的区域随着虚拟机进程的启动而存在，是线程公有的；有些区域依赖于特定的线程而存在，是线程私有的。 程序计数器程序计数器是一小块内存区域，可以看作是当前线程执行的字节码的行号指示器。每条线程都独立拥有一个程序计数器，因此程序计数器是线程私有的。 Java虚拟机栈Java虚拟机栈描述的是Java方法执行的内存模型：每个方法在执行时都会创建一个栈帧（stack frame）用于存储局部变量表、操作数栈、动态链接、方法出口等信息。每个方法从被调用开始执行到执行完成返回的过程，就对应这一个栈帧在虚拟机栈中入栈和出栈的过程。Java内存区域经常被简单地划分为堆内存和栈内存，其中的栈内存粗略来讲就是指的Java虚拟机栈，当然实际上内存划分肯定更加复杂。虚拟机栈与程序计数器一样的线程私有的。局部变量表存放了编译器可知的各种基本数据类型（boolean、byte、char、short、int、float、long、double）、对象引用（reference类型，直观上相当于C语言中的指针，存放的是对象地址）。64位长度的long和double类型的数据会占用2个局部变量空间（Slot），其余的数据类型只占用1个Slot。与Java虚拟机栈相关的两个异常：如果线程请求的栈深度大于虚拟机所允许的深度，就会抛出StackOverflowError异常，一般Java程序中递归调用一个方法而不设置终止条件就会出现这个异常，例如下面样例程序求解斐波那契数列，注释了终止条件就会出现异常；如果虚拟机栈可以动态扩展，但是扩展时无法申请到足够的内存空间，就会抛出OutOfMemoryError异常。 123456public int getFibonacci(int n) &#123; // if(n == 0||n == 1)&#123; // return n; // &#125; return getFibonacciByRecursion(n-1)+getFibonacciByRecursion(n-2);&#125; 本地方法栈本地方法栈（Native Method Stack）与虚拟机栈所发挥的作用相似，只不过区别在于虚拟机栈为虚拟机执行Java方法服务，本地方法栈为虚拟机中使用到的Native方法服务。有的虚拟机，如Sun HotSpot虚拟机将本地方法栈和虚拟机栈合二为一。那什么是Native方法呢？简单来讲，一个native method就是一个java调用非java代码的接口，也就是该方法由非java语言实现，比如C语言。那么可以知道，在定义一个native方法时，不需要提供实现，只需要定义方法名，参数以及返回类型，如下面实例：123456public class IHaveNatives &#123; native public void Native1( int x ) ; native static public long Native2() ; native synchronized private float Native3( Object o ) ; native void Native4( int[] ary ) throws Exception ;&#125; Java堆Java堆（Java Heap）一般是Java虚拟机所管理的内存中最大的一块，被Java虚拟机进程下的所有线程共享的一块内存区域。Java堆的唯一目的就是存放对象实例，几乎所有的对象实例都要在堆上分配内存。Java虚拟机规范中描述：所有的对象实例以及数组都要在堆上分配（The heap is the runtime data area from which memory for all class instances and arrays is allocated），但是随着JIT编译器的发展以及逃逸分析技术的成熟，栈上分配、标量替换优化技术使得所有对象在堆上分配内存就变得不那么“绝对”。Java堆是垃圾收集器管理的主要区域，很多时候也被称之为“GC 堆（Garbage Collected Heap）”。从垃圾回收的角度来看，目前垃圾收集器都采用分代算法，所以Java堆还细分为：新生代和老年代，其中新生代又可以分为Eden空间、From Survivor空间、To Survivor空间，HotSpot虚拟机默认Eden和Survivor的大小比例是8:1。根据Java虚拟机规范规定，Java堆可以处在物理上不连续的内存空间中，只要逻辑连续就行了。目前主流的虚拟机都可以通过命令来自主设置Java堆的大小，其中命令-Xms1024m指的是Java堆的初始大小，-Xmx2048m指的是分配给Java堆的最大内存。 方法区方法区和Java堆一样也是所有线程共享的内存区域，它用于存储已被虚拟机加载的类信息、常量、静态变量、即时编译器编译后的代码等数据。有些在HotSpot虚拟机上开发、部署程序的开发者习惯称方法区为“永久代（Permanent Generation）”，这是由于HotSpot虚拟机的垃圾收集器可以像管理Java堆一样管理方法区这块内存区域，省去了专门为方法区编写内存管理代码的工作。 运行时常量池运行时常量池（Runtime Constant Pool）是方法区的一部分。Class文件中除了有类的版本、字段、方法、接口等描述信息外，还有一项信息是常量池，用于存放编译期生成的各种字面量和符号引用，这部分内容将在类加载后进入方法区的运行时常量池。运行时常量池相对于Class文件常量池的另一个重要特征是具备动态性，Java语言并不要求常量一定只有编译期才能产生，也就是并非预置入Class文件中常量池的内容才能进入方法区运行时常量池，运行期间也可能将新的常量放入池中，比如String类中的intern()方法。 直接内存直接内存（Direct Memory）并不是虚拟机运行时数据区的一部分，也不是Java虚拟机规范中定义的内存区域，但是这部分内存也被频繁使用。自从JDK1.4中引入NIO（New Input/Output）类，提出一种基于通道（Channel）和缓冲区（Buffer）的I/O方式，它可以使用Native方法直接分配堆外内存，然后通过一个存储在Java堆中的DirectByteBuffer对象作为这块内存的引用进行操作。 对象创建过程虚拟机遇到一条new指令，首先检查这个指令的参数是否能在常量池中定位到一个类的符号引用，并且检查这个符号引用代表的类是否已被加载、解析和初始化过。如果没有就必须先执行相关的类加载过程。在类加载检查通过之后，接下来虚拟机将为新生对象分配内存。对象所需内存的大小在类加载完成后就可以完全确定，为对象分配空间的任务就是把一块确定大小的内存从Java堆中划分出来。假设Java堆中内存是绝对规整的，所有用过的内存都放在一边，空闲的内存放在另一边，中间放着一个指针作为分界点的指示器，那么分配内存就是把指针往空闲内存的方向挪动一段与对象大小相等的距离，这种分配方式称为“指针碰撞（Bump the Pointer）”；如果Java堆中的内存不规整，那么虚拟机就必须维护一个列表，记录哪些内存块可用，在为对象分配内存的时候就从列表中寻找一块足够大的空间划分给对象实例，并更新表上的记录，这种分配方式称为“空闲列表（Free List）”。]]></content>
      <categories>
        <category>深入理解Java虚拟机</category>
      </categories>
      <tags>
        <tag>Java堆</tag>
        <tag>方法区</tag>
        <tag>Java虚拟机栈</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[cuda安装]]></title>
    <url>%2F2019%2F01%2F08%2Fcuda%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[cuda安装cuda和cudnn版本查看 cuda版本 1cat /usr/local/cuda/version.txt cudnn版本 1cat /usr/local/cuda/include/cudnn.h | grep CUDNN_MAJOR -A 2 cuda8.0卸载 卸载 1apt autoremove cuda 清除残留文件 12cd /usr/local/rm -rf cuda-8.0/ cuda9.0及对应cudnn安装cuda9.0安装cuda9.0官方网站 运行压缩.run 1sudo sh cuda_9.0.176_384.81_linux.run 一般在不需要图形驱动（Grphics Driver）和 样例（cuda samples） cudnn官方网站 2.~/.bashrc配置 123export CUDA_HOME=/usr/local/cudaexport LD_LIBRARY_PATH=$&#123;CUDA_HOME&#125;/lib64export PATH=$&#123;CUDA_HOME&#125;/bin:$&#123;PATH&#125; 刷新使得配置生效 1source ~/.bashrc cudnn安装本次版本是cuDNN v7.4.2 (Dec 14, 2018), for CUDA 9.0，选择cuDNN Library for Linux。 解压cudnn-9.0-linux-x64-v7.4.2.24.tgz压缩包 1tar -zxvf cudnn-9.0-linux-x64-v7.4.2.24.tgz 复制文件到cuda库下 12cp cuda/lib64/* /usr/local/cuda-9.0/lib64/cp cuda/include/* /usr/local/cuda-9.0/include/ 查看cudnn版本信息 1cat /usr/local/cuda/include/cudnn.h | grep CUDNN_MAJOR -A 2 cuda切换当一台服务器上既安装了cuda8.0和cuda9.0，使用ls命令查看/usr/local下的文件包含三个文件夹cuda、cuda8.0和cuda9.0。 1ls -l /usr/local/ 可以看到，当前cuda文件夹链接到cuda-9.0,当需要切换到cuda8.0时，使用以下命令： 123rm -rf /usr/local/cuda #删除之前创建的软链接sudo ln -s /usr/local/cuda-8.0/ /usr/local/cudanvcc --version #查看当前 cuda 版本 参考博客：http://geyao1995.com/CUDA8_CUDA9/]]></content>
      <categories>
        <category>科研</category>
      </categories>
      <tags>
        <tag>环境配置</tag>
        <tag>软件安装</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hierarchical Object Detection with Deep Reinforcement Learning]]></title>
    <url>%2F2019%2F01%2F08%2FHierarchical-Object-Detection-with-Deep-Reinforcement-Learning%2F</url>
    <content type="text"><![CDATA[Hierarchical Object Detection with Deep Reinforcement LearningAbstract 关键的思想在于关注图像中包含更丰富信息的那些部分并放大它们。 Introduction 考虑区域之间的联系， 利用top-down的扫描方式，首先获取整个图像，关注局部区域的相关信息， 基于增强学习训练的代理（agent）有能力检测图像中的对象 Hierarchical Object Detection Model1. Markov Decision Process（马尔科夫决策过程） State:当前区域描述符（the descriptor of current region）和记忆向量（memory vector） Actions:move actions和terminal actions Reward:保证move action都是朝着更靠近ground truth的方向移动；当IOU超过threshold，则终止移动。 2. Q-learning$$Q(s,a) = r+\lambda{max}_{a’}Q(s’,a’)$$ 3. Model the Image-Zooms model 使用VGG-16提取图像区域特征向量$(77512)$,拼接区域特征向量与记忆向量（memory vector）$(77512+24=25088+24)$,经过两个1024维的全连接层，输出6个可能的动作（actions），反复迭代，直到终止动作 the Pool45-Crops model 4. TrainingExperiments1. Qualitative Results Implementation1. keras实现 提取区域特征 state不断更新，并作为model的输入 1(7*7*512) 代码问题总结 数据类型错误TypeError: slice indices must be integers or None or have an index method 这是由于数组，矩阵等类型数据的下标是整数，而在 12region_mask[offset[0]:offset[0] + size_mask[0] , offset[1]:offset[1] + size_mask[1]] = 1 offset是float类型，所以报错，解决方法就是数据类型转换： 12region_mask[int(offset[0]):int(offset[0] + size_mask[0]) , int(offset[1]):int(offset[1] + size_mask[1])] = 1 VGG16提取图像特征尺寸不对 解决方法：在图片提取特征之前，对图像进行resize； 1im = images[z].resize((224, 224)) 除0错误 对图像进行resize的位置错误 问题总结 问题：记忆向量的哪儿来的？ 问题：哪6个动作？ move actions:左上、右上、左下、右下和中；terminal actions 问题：每个类训练一个模型？ 是的，这篇文章中只训练了飞机类（aeroplane）的检测模型]]></content>
      <categories>
        <category>科研</category>
      </categories>
      <tags>
        <tag>对象检测</tag>
        <tag>论文阅读</tag>
        <tag>神经网络</tag>
        <tag>图像识别</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Yotta系统错误记录及性能优化]]></title>
    <url>%2F2019%2F01%2F07%2FYotta%E7%B3%BB%E7%BB%9F%E9%94%99%E8%AF%AF%E8%AE%B0%E5%BD%95%E5%8F%8A%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%2F</url>
    <content type="text"><![CDATA[错误记录 在日志系统运行期间，每隔几天kafka平台就会自动崩溃，控制台窗口关闭，并且重新启动kafka平台也会出现日志文件冲突问题，从而导致日志系统无法正常记录用户行为日志。 解决方法： 首先，我们查阅了官网的各种文档，在全面了解kafka的实现原理与应用之后仍然没有找到解决问题的方法。然后我们在StackOverflow和Quora上进行了相关问题的查看，尝试后发现类似问题解决方法，将kafka平台对应的日志文件删除就可以正常启动kafka平台。虽然通过删除日志文件的方式使得日志记录正常进行，但是依旧没有解决每隔几天kafka平台就会自动崩溃的问题，频繁人工启动不稳定的服务耗费人力开销。 为了彻底解决这个问题，我们维护人员仔细阅读kafka生成的日志文件，了解到崩溃的发生是因为日志文件大小达到阈值时kafka程序会对日志文件重命名，而此时日志文件又被自身所占用，就产生了程序异常，在Apache官网issues下发现这是此版本kafka在windows下的一个错误，所以我们将日志系统整体迁移到linux系统下，将问题完美解决。 性能优化 Optimizing MySQL LIKE ‘%string%’ queries 方法1：建立fulltext索引，全文索引只能用于数据库引擎为MYISAM的数据表，但是全文索引不支持中文； 方法2：]]></content>
      <categories>
        <category>工程</category>
      </categories>
      <tags>
        <tag>yotta系统</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[图像文本匹配相关工作]]></title>
    <url>%2F2018%2F11%2F07%2F%E5%9B%BE%E5%83%8F%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[图像文本匹配相关工作introduction 什么是图像文本匹配？计算机视觉任务逐渐不在满足于简单的图像分类、或者为图像分配一个或几个标签的任务，越来越多的研究者希望能够通过匹配图像和文本，为图像生成丰富的文本描述，从而更好地理解图像的语义。 已经有大量的研究工作，这些工作的方法怎么做的？ 我的工作在别人的工作上有什么改进？有什么优点、贡献？ 通过生成网络生成更多的正例(positive fact), 从而扩充训练数据集; 设计了一个高效的训练算法，交替优化生成网络和判别网络的参数，得到强判别器； 在多个数据集上实现了与其他方法可比较甚至更优异的结果。 related work1. CCA Canonical Correlation Analysis (CCA) Kernel Canonical Correlation Analysis (KCCA) deep CCA Sparse Kernel CCA Randomized CCA Nonparametric CCA 2. ranking based method Y. Verma and C. Jawahar, “Im2text and text2im: Associating images and texts for cross-modal retrieval,” in British Machine Vision Conference (BMVC), vol. 1, 2014, p. 2. R. Socher, A. Karpathy, Q. V. Le, C. D. Manning, and A. Y. Ng, “Grounded compositional semantics for finding and describing images with sentences,” Transactions of the Association for Computational Linguistics, vol. 2, pp. 207–218, 2014. OK A. Karpathy, A. Joulin, and F. F. F. Li, “Deep fragment embeddings for bidirectional image sentence mapping,” in Neural Information Processing Systems (NIPS), 2014, pp. 1889–1897. OK R. Kiros, R. Salakhutdinov, and R. S. Zemel, “Unifying visual-semantic embeddings with multimodal neural language models,” arXiv preprint arXiv:1411.2539, 2014. OK L. Wang, Y. Li, and S. Lazebnik, “Learning deep structure-preserving image-text embeddings,” in Computer Vision and Pattern Recognition (CVPR), 2016, pp. 5005–5013. “Learning two-branch neural networks for image-text matching tasks,” arXiv preprint arXiv:1704.03470, 2017. Huang Y, Wang W, Wang L. Instance-aware image and sentence matching with selective multimodal lstm[C]//The IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 2017, 2(6): 7. Huang Y, Wu Q, Wang L. Learning semantic concepts and order for image and sentence matching[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018: 6163-6171. 3.Generative Adversarial Networks(GANs) @inproceedings{goodfellow2014generative,title={Generative adversarial nets},author={Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},booktitle={Advances in neural information processing systems},pages={2672–2680},year={2014}} @article{mirza2014conditional,title={Conditional generative adversarial nets},author={Mirza, Mehdi and Osindero, Simon},journal={arXiv preprint arXiv:1411.1784},year={2014}} @article{radford2015unsupervised,title={Unsupervised representation learning with deep convolutional generative adversarial networks},author={Radford, Alec and Metz, Luke and Chintala, Soumith},journal={arXiv preprint arXiv:1511.06434},year={2015}} @article{reed2016generative,title={Generative adversarial text to image synthesis},author={Reed, Scott and Akata, Zeynep and Yan, Xinchen and Logeswaran, Lajanugen and Schiele, Bernt and Lee, Honglak},journal={arXiv preprint arXiv:1605.05396},year={2016}} @inproceedings{wang2017irgan,title={Irgan: A minimax game for unifying generative and discriminative information retrieval models},author={Wang, Jun and Yu, Lantao and Zhang, Weinan and Gong, Yu and Xu, Yinghui and Wang, Benyou and Zhang, Peng and Zhang, Dell},booktitle={Proceedings of the 40th International ACM SIGIR conference on Research and Development in Information Retrieval},pages={515–524},year={2017},organization={ACM}} @article{cai2017kbgan,title={Kbgan: Adversarial learning for knowledge graph embeddings},author={Cai, Liwei and Wang, William Yang},journal={arXiv preprint arXiv:1711.04071},year={2017}} experiment 数据集的扩充：对每一张图片进行裁剪，4个角以及中间，并将这5个裁剪的图翻转，一张图片扩充得到10张尺寸为$128 \times 128$的图片。]]></content>
      <categories>
        <category>科研</category>
      </categories>
      <tags>
        <tag>图文匹配</tag>
        <tag>典型相关分析</tag>
        <tag>排序损失</tag>
        <tag>数据集</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[研究生数学建模：恐怖袭击事件分级]]></title>
    <url>%2F2018%2F09%2F15%2F%E7%A0%94%E7%A9%B6%E7%94%9F%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%EF%BC%9A%E6%81%90%E6%80%96%E8%A2%AD%E5%87%BB%E4%BA%8B%E4%BB%B6%E5%88%86%E7%BA%A7%2F</url>
    <content type="text"><![CDATA[研究生数学建模：恐怖袭击事件分级特征：人员伤亡、经济损失、时间、地点（人口容量、GDP）等 财产损失特征：攻击类型、武器类型、受害子类型、国家、地区、入选标准（1、2、3、doubtterr） 攻击类型 暗杀（1/0） 武装袭击（1/0） 轰炸爆炸（1/0） 劫持（1/0） 设施攻击（1/0） 徒手攻击（1/0） 未知（1/0） 攻击成功（1/0） 自杀式袭击（suicide）武器类型 生化武器、放射性武器（1/0） 核武器 轻武器 炸弹 燃烧武器 治乱武器 交通工具 破坏设备 未知 受害者类型 商业 政府 警察 军事 流产有关 运输（机场（飞机）或巴士、火车、高铁运输） 教育机构 食物或水供应 媒体设施 海事 非政府组织 其他 地区 北美 南美 东亚 东南亚 南亚 中亚 西欧 东欧 中东和北非 撒哈拉以南非洲 澳大利亚 入选标准 标准1 标准2 标准3 – 疑似恐怖主义 输出： 灾难性的 重大的 较小的 无损失 未知 设置propextent中为空的部分为0，这样就可以去除property K-means聚类结果分析 样本数110953\times 17 聚类结果（5类） 统计 类1 类2 类3 类4 类5 数量 2497 4645 134 1854 1823 Spectral Clustering聚类结果分析 样本数110953\times 17 聚类结果（5类） 统计 类1 类2 类3 类4 类5 数量 10883 1 15 1 53 分段后K-means聚类结果分析 样本数110953\times 36 聚类结果（5类） 统计 类1 类2 类3 类4 类5 数量 6339 1541 1474 725 874 分段后Spectral Clustering聚类结果分析 样本数110953\times 36 聚类结果（5类） 统计 类1 类2 类3 类4 类5 数量 9472 26 1343 93 19 增大死亡数以及财产损失权重，分段后K-means聚类结果分析 样本数110953\times 36 聚类结果（5类） 统计 类1 类2 类3 类4 类5 数量 1541 735 7203 1474 0 增大死亡数以及财产损失权重，分段后Spectral Clustering聚类结果分析 样本数110953\times 36 聚类结果（5类） 统计 类1 类2 类3 类4 类5 数量 9471 1 1349 39 93]]></content>
      <categories>
        <category>竞赛</category>
      </categories>
      <tags>
        <tag>编程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[智慧教育师范应用表格接口规范]]></title>
    <url>%2F2018%2F07%2F10%2F%E6%99%BA%E6%85%A7%E6%95%99%E8%82%B2%E5%B8%88%E8%8C%83%E5%BA%94%E7%94%A8%E8%A1%A8%E6%A0%BC%E6%8E%A5%E5%8F%A3%E8%A7%84%E8%8C%83%2F</url>
    <content type="text"><![CDATA[智慧教育-网络学院示范应用后端数据访问接口主题状态1. 主题状态表 列名 类型 长度 state_id bigint(long) 20 domain_id bigint(long) 20 states varchar(string) 255 user_id bigint(long) 20 created_time datetime 1 modified_time datetime 1 说明： （1） 学习状态：0表示未学习，1表示正在学习，2表示已学习； （2） states（主题状态列表）形式：学习状态1，学习状态2，学习状态3，…… 举例： 1， 0， 1， 2，…… 2. 主题状态接口 （1） /topicState/getByDomainIdAndUserId 查询主题状态，参数 long domainId long userId 课程id 用户id （2） /topicState/saveStateByDomainIdAndUserId 保存主题状态，参数 long domainId String states long userId 课程id 主题状态列表 用户id （3） /topicState/saveStateByDomainNameAndUserId 保存主题状态，参数 long domainName String states long userId 课程名 主题状态列表 用户id 分面状态1. 分面状态表 列名 类型 长度 state_id bigint(long) 20 domain_id bigint(long) 20 topic_id bigint(long) 20 states varchar(string) 255 user_id bigint(long) 20 created_time datetime 1 modified_time datetime 1 说明： （1） 学习状态：0表示未学习，1表示已在学习； （2） states（分面状态列表）形式：学习状态1，学习状态2，学习状态3，…… 举例： 1， 0， 1， 0，…… 2. 分面状态接口 （1）/facetState/getByDomainIdAndTopicIdAndUserId 查询分面状态，参数 long domainId long userId long topicId 课程id 用户id 主题id （2）/facetState/saveStateByDomainIdAndTopicIdAndUserId 保存分面状态，参数 long domainId long topicId String states long userId 课程id 主题id 分面状态列表 用户id （3）/facetState/saveStateByDomainNameAndTopicNameAndUserId 保存分面状态，参数 string domainName string topicName String states long userId 课程名 主题名 分面状态列表 用户id （4）/facetState/saveStateByDomainIdAndUserId 保存分面状态，参数 long domainId String states long userId 课程id 分面状态列表 用户id （5）/facetState/saveStateByDomainNameAndUserId 保存主题状态，参数 long domainName String states long userId 课程名 分面状态列表 用户id 注：states：分面状态的矩阵（行（主题）之间以分号隔开，行内以逗号隔开） 例如：0,0,1,0;0,0,1;1,1,0;1,0,1,1,1;……推荐主题1. 推荐主题表 列名 类型 长度 recommendation_id bigint(long) 20 domain_id bigint(long) 20 recommendation_topics varchar(string) 255 user_id bigint(long) 20 created_time datetime 1 modified_time datetime 1 说明： （1）recommendation_topics（推荐主题列表）形式：：推荐主题1 id，推荐主题2 id，推荐主题3 id；推荐主题3 id，推荐主题1 id，推荐主题4 id；……即，不同推荐方式之间以分号隔开，同一推荐方式内的主题id以逗号分隔开 2. 2. 推荐主题接口 （1） recommendation/getByDomainIdAndUserId 查询推荐主题，参数 long domainId long userId 课程id 用户id （2） recommendation/saveRecommendationByDomainIdAndUserId 保存推荐主题，参数 long domainId String recommendationTopics long userId 课程id 推荐主题列表 用户id （3） recommendation/saveRecommendationByDomainNameAndUserId 保存推荐主题，参数 long domainName String recommendationTopics long userId 课程名 推荐主题列表 用户id]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>数据库</tag>
        <tag>智慧教育系统</tag>
      </tags>
  </entry>
</search>

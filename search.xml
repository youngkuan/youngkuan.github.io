<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[并发编程的挑战]]></title>
    <url>%2F2019%2F01%2F20%2F%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E7%9A%84%E6%8C%91%E6%88%98%2F</url>
    <content type="text"><![CDATA[并发编程的挑战上下文切换在多线程执行代码的过程中，CPU为每个线程分配CPU时间片保证线程的执行，时间片很短，一般是几十毫秒（ms）。CPU通过时间片分配算法来循环执行任务，当前任务执行一个时间片后会切换到下一个任务。但是，在切换前会保存前一个任务的状态，以便下次切换回这个任务的时候可以再加载这个任务。任务从保存到再加载的过程就是一次上下文切换。多线程由于上下文切换的存在使得其不一定比单线程快，一般减少上下文切换的方法有无锁并发编程、CAS算法、使用最少线程和使用协程。 无锁并发编程：多线程竞争锁时，会引起上下文切换，所以多线程处理数据时，可以用一些方法来避免使用锁，如将数据的ID按照Hash算法取模分段，不同线程处理不同段的数据； CAS算法：Java的Atomic包使用CAS算法来更新数据，而不需要加锁； 使用最少线程：避免创建不需要的线程，比如任务少时，创建很多线程会造成大量线程都处于等待状态； 使用协程：在单线程中实现多任务的调度，并在单线程里维持多个任务间的切换。死锁死锁是指由于两个或者多个任务互相持有对方所需要的资源，导致这些线程处于等待状态，无法前往执行。 线程死锁产生的四个必要条件 互斥条件：某种资源一次只允许一个线程访问，即该资源一旦分配给某个线程，其他线程就不能再访问，直到该进程访问结束并释放； 请求和保持条件：指线程已经保持至少一个资源，但又提出了新的资源请求，而该资源已被其它线程占有，此时请求进程阻塞，但又对自己已获得的其它资源保持不放； 不剥夺条件：指线程已获得的资源，在未使用完之前，不能被剥夺，只能在使用完时由自己释放； 环路(循环)等待条件：指在发生死锁时，必然存在一个线程——资源的环形链，即线程集合{t0，t1，t2，···，tn}中的t0正在等待一个t1占用的资源；t1正在等待t2占用的资源，……，tn正在等待已被t0占用的资源。 避免死锁的几个常用方法 避免一个线程同时获得多个锁； 避免一个线程在锁内同时占用多个资源，尽量保证每个锁只占用一个资源； 尝试使用定时锁，使用lock。tryLock(timeout)来替代使用内部锁机制； 对于数据库锁，加锁和解锁必须在一个数据库连接里，否则会出现解锁失败的情况。 资源限制的挑战资源限制是指在进行并发编程时，程序的执行速度受限于计算机硬件资源或软件资源。例如，网络带宽的限制、硬盘读写速度的限制、CPU计算速度的限制、数据库的连接数和socket连接数限制等。 资源限制引发的问题在并发编程中，将代码执行速度加快的原则是将代码中串行执行的部分变为并发执行，但如果将某段串行代码并发执行，因为资源受限，仍然在串行执行，这时候程序不仅不会加快执行，反而会更慢，因为增加了上下文切换和资源调度的时间。 资源限制的解决对应硬件资源限制，可以考虑使用集群并行执行程序。既然单机的资源有限制，那么就让程序在多机上运行。比如用ODPS、Hadoop或者自己搭建服务器集群，不同的机器处理不同的数据。对于软件资源限制，可以考虑使用资源池复用资源。比如使用连接池将数据库和Socket连接复用，或者在调用对方webservice接口获取数据时，只建立一个连接。 资源限制下的并发编程根据不同的资源限制调整程序的并发度，比如下载文件程序依赖于两个资源——带宽和硬盘读写速度。有数据库操作时，涉及数据库连接数，如果SQL语句执行非常快，而线程的数量比数据库连接数大很多，就会导致某些线程被阻塞，等待数据库连接，因此这个时候就应该降低线程数量。]]></content>
      <categories>
        <category>并发编程</category>
      </categories>
      <tags>
        <tag>上下文切换</tag>
        <tag>死锁</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kafka:分布式流平台]]></title>
    <url>%2F2019%2F01%2F16%2FKafka-%E5%88%86%E5%B8%83%E5%BC%8F%E6%B5%81%E5%B9%B3%E5%8F%B0%2F</url>
    <content type="text"><![CDATA[Kafka:分布式流平台参考链接：Kafka官网 Apache Kafka是一个分布式流平台，具有以下三个关键能力： 发布和订阅记录流（streams of records），类似于一个消息队列或者是企业消息系统； 以具有容错性和持久性的方式存储记录流； 即时处理记录流 Kafka通常被用在两大类应用中： 构建可在系统或应用程序之间可靠获取数据的实时流数据管道 构建转换或响应数据流的实时流应用 一些概念： Kafka作为一个集群可以运行在一个或多个服务器上，这些服务器可跨多个数据中心； Kafka集群存储在类别中的记录流称为主题（topics）； 每个记录包含一个键（key）、一个值（value）以及一个时间戳（timestamp）； Kafka有四个核心API： 生产者（Producer）API允许一个应用将记录流发布到一个或多个Kafka主题（topics）上； 消费者（Consumer）API允许一个应用去订阅一个或者多个主题（topics），并处理这些给它们生产的记录流； 流（Streams）API允许一个应用扮演流处理器的角色，从一个或多个主题消费一个输入流，然后向一个或多个主题输出流，高效地进行输入流到输出流的转换； 连接（Connector）API允许创建或者运行可重用的生产者或消费者，并与将Kafka主题与现有的应用或数据系统连接。例如关系数据库的连接器可以捕获对表的每个更改。 主题（Topics）和记录（Logs）一个主题是发布记录的一个类别或订阅源名称。 Kafka中主题总是多订阅用户; 也就是说，一个主题可以有零个，一个或多个消费者订阅写入它的数据。对于每个主题，Kafka群集都维护一个如下所示的分区日志（partitioned log）： 每个分区（partition）都是有序的，不可变的记录序列，这些记录不断添加到结构化的提交日志中。分区中的每个记录都被分配一个称为偏移的序列id，这些id唯一标识了分区中的每个记录。Kafka集群使用可配置的保留期，持久保存所有已发布的记录，无论这些记录是否已被使用。 例如，如果保留策略被设置为两天，那么在发布记录后的两天内，记录都可以被消费，之后将被丢弃以释放空间。 Kafka的性能不受数据大小的影响，因此长时间存储数据不是问题。实际上，基于每个消费者保留的唯一元数据是该消费者在日志中的偏移或位置。 这种偏移由消费者控制：通常消费者会线性地提高其偏移量从而读取记录，但事实上，由于该位置由消费者控制，因此它可以按照自己喜欢的任何顺序去消费记录。 例如，消费者可以重置为较旧的偏移量去重新处理过去的数据，或者跳到最近的记录并从“现在”开始消费。 这些特性的组合意味着Kafka消费者非常方便，消费者的加入移除对集群或其他消费者没有太大影响。 例如，您可以使用我们的命令行工具“拖尾（tail，此处不太理解）”任何主题的内容，而无需更改任何现有使用者所消耗的内容。 日志中的分区有多种用途。 首先，它们允许日志扩展到超出适合单个服务器的规模。 每个独立分区必须适合托管它的服务器，但一个主题可能有多个分区，因此它可以处理任意数量的数据。 其次，分区充当了并行性的单元。]]></content>
      <categories>
        <category>kafka</category>
      </categories>
      <tags>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[图片服务器搭建]]></title>
    <url>%2F2019%2F01%2F14%2F%E5%9B%BE%E7%89%87%E6%9C%8D%E5%8A%A1%E5%99%A8%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[图片服务器搭建环境：window server 2016 Datacenter 首先下载nginx软件包，我下在的版本是nginx-1.15.8，下载链接和英文安装教程如下: nginx下载地址 nginx英文安装教程 解压nginx-1.15.8.zip，目录结构如下图所示： 运行nginx.exe,并查看通过命令查看运行状态，如下所示： 如果没有运行成功，可以通过打开logs\error.log查看错误记录。 打开浏览器，输入localhost测试是否成功，如下图所示： 搭建图片服务器 首先打开conf目录下的nginx.conf配置文件，修改如下： 重新启动nginx，一些命令如下： 1234nginx -s stop fast shutdownnginx -s quit graceful shutdownnginx -s reload changing configuration, starting new worker processes with a new configuration, graceful shutdown of old worker processesnginx -s reopen re-opening log files 此时在images文件夹下放一张随意图片（我放了一张nginx网站的截图），打开浏览器，输入链接http://47.95.145.72:8090/1.png，就可以看到我们的图片，如下所示： 到这里，图片服务器就搭建完成了！]]></content>
      <categories>
        <category>工程</category>
      </categories>
      <tags>
        <tag>nginx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[垃圾收集（二）]]></title>
    <url>%2F2019%2F01%2F14%2F%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%EF%BC%88%E4%BA%8C%EF%BC%89%2F</url>
    <content type="text"><![CDATA[垃圾收集垃圾收集算法标记-清除算法算法分为标记阶段和清除阶段 ，首先标记出所有需要回收的对象，在标记完成后统一回收所有被标记的对象，它的标记过程就是引用计数法或者可达性算法判断对象是否存活的过程。不足： 效率问题，标记和清除两个阶段的效率都不高； 空间问题，标记清除之后会产生大量不连续的内存碎片，空间碎片太多可能会导致以后程序运行过程中需要分配较大对象时，无法找到足够的内存碎片而不得不提前出发一次垃圾收集动作。标记清除算法的执行过程如下图所示： 复制算法为了解决效率问题，一种“复制”的垃圾收集算法出现了，它将可用的内存分成大小相等的两个区域，每次只使用其中的一块区域。当这块区域用完之后，就将还存活着的对象复制到另一块内存区域上，然后再把已使用过的内存空间一次清理掉。这样使得每次都是对整个半区进行垃圾回收，内存分配时也就不用再考虑内存碎片等复杂情况，只要移动堆顶指针，按顺序分配内存即可，实现简单，运行高效。只是这种算法每次只能使用一半的内存，相当于将内存缩小为原来的一半，代价太高。复制算法的执行过程如下图所示： 目前的商业虚拟机都是采用复制算法回收新生代，但是并不按照1:1划分内存空间，而是将内存分为一块较大的Eden空间和两块较小的Survivor空间，每次使用Eden和其中一块Survivor。当回收时，将Eden和Survivor中还存活的对象一次性复制到另一块Survivor空间上，最后清理掉Eden和刚才用过的Survivor空间。HotSpot虚拟机默认Eden：Survivor=8:1，也就是每次新生代中可用内存空间为整个新生代容量的90%，只有10%的内存被“浪费”。当然在大多数（98%）情况下，只有不超过10%的对象存活，而当超过10%时，也就意味着Survivor空间不够用，这个时候就需要依赖老年代进行分配担保（无法再Survivor区存放的对象直接进入老年代）。 标记-整理算法复制算法在对象存活率较高的情况下就会进行较多的复制操作，效率将会变低，而且还需要额外的内存空间进行分配担保，以应对内存（Survivor空间）不够容纳存活对象的极端情况。因此，老年代一般不采用复制算法，而采用“标记-整理”（Mark-Compact）算法，标记过程仍然与“标记-清除”算法一样，但后续步骤不是直接对可回收对象进行清理，而是让存活对象都向一端移动，然后直接清理掉端边界以外的内存，标记-整理算法的示意图如下： 分代收集算法当前商业虚拟机的垃圾收集都采用“分代收集”（Generational Collection）算法，这种算法根据对象存活周期的不同将内存划分为几块。一般是把Java堆划分成新生代和老生代，从而根据各个年代的特点采用最适当的收集算法。在新生代中，每次垃圾收集都会发现大量的对象死去，只有少量对象存活，因此采用复制算法，只需要付出少量存活对象的复制成本就可以完成收集；在老年代中因为对象存活率高、没有额外的空间对它进行分配担保，就必须采用“标记-清理”或者“标记-整理”算法进行回收。 HotSpot的算法实现枚举根节点从可达性分析中从GC Roots节点找到引用链这个操作为例，可作为GC Roots的加点主要在全局性的引用（例如常量或类静态变量）与执行上下文（例如栈帧中的本地变量表）中，现在的很多应用仅仅方法区就有数百兆，如果要逐个检查这里面的引用，那么必然会消耗很多时间。另外，可达性分析对执行时间的敏感还体现在GC停顿上，因为这项分析工作必须在一个能确保一致性的快照中进行——这里的“一致性”的意思是在真个分析过程中整个执行系统看起来就像被冻结在某个时间点上，不可以出现分析过程中对象引用关系还在不断变化的情况，该点不满足的话分析结果准确性就无法得到保证。这点是导致在GC进行时必须停止多有Java执行线程的其中一个重要原因。即时在号称（几乎）不会发生停顿的CMS收集器中，枚举根节点时也是必须要停顿的。由于目前主流的Java虚拟机使用的都是准确式GC，所以当执行系统停顿下来后，并不需要一个不漏地检查完所有执行上下文和全局的引用位置，虚拟机应当是有办法直接得知哪些地方存放着对象引用。在HotSpot的实现中，是使用一组称为OopMap的数据结构来达到这个目的，在类加载完成时，HotSpot就把对象内什么偏移量上是什么类型的数据计算出来，在JIT编译过程中，也会在特定位置记录下栈和寄存器哪些位置是引用。 安全点在OopMap的协助下，HotSpot可以快速且准确地完成GC Roots枚举，但一个很现实的问题随之而来：可能导致引用关系发生变化，或者说OopMap内容变化的指令非常多，如果每一条指令都生成对应的OopMap，那将会需要大量的额外空间，这样GC的空间成本就会变得很高。实际上，HotSpot也的确没有为每条指令都生成OopMap，前面已经提到只是在“特定的位置”记录这些信息，这些位置称为安全点（Safepoint），即程序执行时并非在所有的地方都能停顿下来开始GC，只有到达安全点时才能暂停。Safepoint的选定既不能太少以致于GC等待时间过长，也不能过于频繁以致于过分增大运行时的负荷。所以，安全点的选定基本上是以程序“是否具有让程序长时间执行的特征”为标准进行选定的————因为每一条指令执行的时间都非常短暂，程序不太可能因为指令流长度太长这个原因而过长时间运行，“长时间执行”最明显的特征就是指令序列复用，例如方法调用、循环跳转、异常跳转等，所以具有这些功能的指令才回产生Safepoint。对于Safepoint，另一个需要考虑的问题就是如何在GC发生时让所有的线程都“跑”到最近的安全点上停顿下来。这里有两种方案可以选择:抢先式中断（Preemptive Suspension）和主动式中断（Voluntary Suspension），其中抢先式中断不需要线程的执行代码主动去配合，在GC发生时，首先把所有的线程全部中断，如果发现线程中断的地方不在安全点上，就恢复线程，让它“跑”到安全点上。现在几乎没有虚拟机实现采用抢先式中断来暂停线程从而相应GC时间。而主动式中断的思想是当GC需要中断线程的时候，不直接对线程操作，仅仅简单地设一个标志，各个线程执行时主动去轮询这个标志，发现中断标志为真时就自己中断挂起。 安全区域使用Safepoint似乎已经完美解决了如何进入GC的问题，但实际情况却并不一定。Safepoint机制保证了程序执行时，在不太长的时间内就会遇到可进入GC的Safepoint。但是程序不执行的时候，CPU没有分配时间给程序，典型例子就是线程处于Sleep或者Blocked状态，这时线程无法响应JVM的中断请求，“走”到安全的地方去挂起中断，JVM也不太可能等待这些线程重新分配CPU时间。对于这种情况就需要安全区域（Safe Region）来解决。安全区域是指在一段代码片段中，引用关系不会发生变化。在这个区域中的任何地方开始GC都是安全的。在线程执行到Saferegion中的代码时，首先标志自己进入了Saferegion，那样当在这段时间里JVM要发起GC时，就不用管标志自己为Saferegion状态的线程了。在线程要离开Saferegion时，它要检查系统是否已经完成了根节点枚举（或者整个GC过程），如果完成了那么线程继续执行，否则它就要等待直到收到可以安全离开Saferegion的信号为止。]]></content>
      <categories>
        <category>深入理解Java虚拟机</category>
      </categories>
      <tags>
        <tag>垃圾收集算法</tag>
        <tag>垃圾收集器</tag>
        <tag>安全点</tag>
        <tag>安全区域</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[垃圾收集(一)]]></title>
    <url>%2F2019%2F01%2F12%2F%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%2F</url>
    <content type="text"><![CDATA[垃圾收集垃圾回收需要关注的事情： 哪些内存需要回收？ Java堆和方法区 什么时候回收？ 如何回收？ java内存运行时各个区域，其中程序计数器、java虚拟机栈、本地方法栈3个区域随线程而生，随线程而灭；栈中的栈帧随着方法的进入和退出而有条不紊地进行出栈和入栈操作。而Java堆和方法区则不一样，一个接口中的多个实现类需要的内存可能不一样，一个方法中的多个分支需要的内存也可能不一样，我们只有在程序处于运行期间时才能知道会创建哪些对象，这部分内存的分配和回收都是动态的，垃圾回收器所关注的就是这部分内存。 对象存活判断引用计数法（Reference Counting）算法描述：给对象中添加一个引用计数器，每当有一个引用指向这个对象，计数器值就加1；当引用失效时，计数器值就减1；任何时刻计数器值为0的对象就不可能再被使用。引用计数法存在一个问题：它很难解决对象之间的相互循环引用问题，举个例子，示例代码如下。对象123456789101112131415161718192021222324252627```/** * 对象objA和objB存在相互引用 * @author yangkuan */public class ReferenceCounteringGC &#123; public Object instance = null; private static final int _1MB = 1024 * 1024; /** * 这个成员变量的意义是通过其占用的内存，通过GC日志查看对象是否被回收 */ private byte[] bigSize = new byte[2 * _1MB]; public static void testGC()&#123; ReferenceCounteringGC objA = new ReferenceCounteringGC(); ReferenceCounteringGC objB = new ReferenceCounteringGC(); objA.instance = objB; objB.instance = objA; objA = null; objB = null; // 假设此处发生垃圾回收，如果回收算法是引用计数法，那么objA和objB将不会被回收 System.gc(); &#125;&#125; 可达性分析算法（Reachability Analysis）算法描述这个算法的基本思想就是通过一系列的称为“GC Roots”的对象作为起始点，从这些节点开始向下搜索，搜索所走过的路径称为引用链（Reference Chain），当一个对象到GC Roots没有任何引用链相连（即该对象到GC Roots不可达）时，则证明这个对象不可用。那这些不可达的对象就可以判定为可回收对象。可作为GC Roots的对象包括以下几种： 虚拟机栈（栈帧中的本地变量表）中引用的对象； 方法区中类静态属性引用的对象； 方法区中常量引用的对象； 本地方法栈中JNI（即Native方法）引用的对象。 引用类型 强引用（Strong Reference）强引用就是指在程序代码之中普遍存在的，类似Object obj = new Object()这类的引用，只要强引用还存在，垃圾收集器永远不会回收掉被引用的对象。 软引用（Soft Reference）软引用是用来描述一些还有用但并非必须的对象。对于软引用关联着的对象，在系统将要发生内存溢出异常之前，将会把这些对象列进回收范围之中进行第二次回收。如果这次回收之后还没有足够的内存，那么程序就会抛出内存溢出异常。 弱引用（Weak Reference）弱引用也是用来描述非必须对象的，但是它的强度比软引用还要更弱一些，被弱引用关联的对象只能生存到下一次垃圾回收之前。当垃圾收集器工作时，无论当内存是否足够，都会回收掉只被弱引用关联的对象。 虚引用（Phantom Reference）虚引用也被称为幽灵引用或者幻影引用，它是最弱的一种引用关系。一个对象是否有虚引用的存在，完全不会对其生存时间构成威胁，也无法通过虚引用来取得一个对象实例。为一个对象设置虚引用关联的唯一目的就是能在这个对象被收集器回收时收到一个系统通知。 对象的两次标记过程 如果对象再进行可达性分析的时候发现其与GC Roots之间不可达，那么它将会被第一次标记并进行下一次筛选，筛选的条件是此对象是否有必要执行finalize()方法。当对象没有覆盖finalize()方法或者finalize()方法已经被虚拟机执行过，都会被认为没有必要执行。 如果这个对象被虚拟机认为有必要执行finalize()方法，那么这个对象将会放置在一个叫F-Queue的队列中，并在稍后由一个由虚拟机自动建立的、低优先级的Finalizer线程去执行它。如果对象想要拯救自己，那么覆盖finalize()方法在方法中重新将自身与引用链上的任意一个对象关联起来就可以避免自己被回收。 回收方法区方法区（永久代）的垃圾回收主要包括两个部分：废弃常量和无用的类。 废弃常量指的是没有任何地方引用这个常量，这个常量会被系统清理出常量池； 无用的类需要同时满足以下三个条件： 该类的所有实例都已经被回收，也就是Java堆中不存在该类的任何实例； 加载该类的ClassLoader已经被回收； 该类对应的java.lang.Class对象没有在任何地方被引用，无法再任何地方通过反射访问该类的方法。]]></content>
      <categories>
        <category>深入理解Java虚拟机</category>
      </categories>
      <tags>
        <tag>内存分配策略</tag>
        <tag>引用计数法</tag>
        <tag>可达性算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java内存区域]]></title>
    <url>%2F2019%2F01%2F10%2FJava%E5%86%85%E5%AD%98%E5%8C%BA%E5%9F%9F%2F</url>
    <content type="text"><![CDATA[Java内存区域Java虚拟机在执行Java程序的过程中，把其所管理的内存划分成多个区域，如下图所示。每个数据区域都有各自的用途，有的区域随着虚拟机进程的启动而存在，是线程公有的；有些区域依赖于特定的线程而存在，是线程私有的。 程序计数器程序计数器是一小块内存区域，可以看作是当前线程执行的字节码的行号指示器。每条线程都独立拥有一个程序计数器，因此程序计数器是线程私有的。 Java虚拟机栈Java虚拟机栈描述的是Java方法执行的内存模型：每个方法在执行时都会创建一个栈帧（stack frame）用于存储局部变量表、操作数栈、动态链接、方法出口等信息。每个方法从被调用开始执行到执行完成返回的过程，就对应这一个栈帧在虚拟机栈中入栈和出栈的过程。Java内存区域经常被简单地划分为堆内存和栈内存，其中的栈内存粗略来讲就是指的Java虚拟机栈，当然实际上内存划分肯定更加复杂。虚拟机栈与程序计数器一样的线程私有的。局部变量表存放了编译器可知的各种基本数据类型（boolean、byte、char、short、int、float、long、double）、对象引用（reference类型，直观上相当于C语言中的指针，存放的是对象地址）。64位长度的long和double类型的数据会占用2个局部变量空间（Slot），其余的数据类型只占用1个Slot。与Java虚拟机栈相关的两个异常：如果线程请求的栈深度大于虚拟机所允许的深度，就会抛出StackOverflowError异常，一般Java程序中递归调用一个方法而不设置终止条件就会出现这个异常，例如下面样例程序求解斐波那契数列，注释了终止条件就会出现异常；如果虚拟机栈可以动态扩展，但是扩展时无法申请到足够的内存空间，就会抛出OutOfMemoryError异常。 123456public int getFibonacci(int n) &#123; // if(n == 0||n == 1)&#123; // return n; // &#125; return getFibonacciByRecursion(n-1)+getFibonacciByRecursion(n-2);&#125; 本地方法栈本地方法栈（Native Method Stack）与虚拟机栈所发挥的作用相似，只不过区别在于虚拟机栈为虚拟机执行Java方法服务，本地方法栈为虚拟机中使用到的Native方法服务。有的虚拟机，如Sun HotSpot虚拟机将本地方法栈和虚拟机栈合二为一。那什么是Native方法呢？简单来讲，一个native method就是一个java调用非java代码的接口，也就是该方法由非java语言实现，比如C语言。那么可以知道，在定义一个native方法时，不需要提供实现，只需要定义方法名，参数以及返回类型，如下面实例：123456public class IHaveNatives &#123; native public void Native1( int x ) ; native static public long Native2() ; native synchronized private float Native3( Object o ) ; native void Native4( int[] ary ) throws Exception ;&#125; Java堆Java堆（Java Heap）一般是Java虚拟机所管理的内存中最大的一块，被Java虚拟机进程下的所有线程共享的一块内存区域。Java堆的唯一目的就是存放对象实例，几乎所有的对象实例都要在堆上分配内存。Java虚拟机规范中描述：所有的对象实例以及数组都要在堆上分配（The heap is the runtime data area from which memory for all class instances and arrays is allocated），但是随着JIT编译器的发展以及逃逸分析技术的成熟，栈上分配、标量替换优化技术使得所有对象在堆上分配内存就变得不那么“绝对”。Java堆是垃圾收集器管理的主要区域，很多时候也被称之为“GC 堆（Garbage Collected Heap）”。从垃圾回收的角度来看，目前垃圾收集器都采用分代算法，所以Java堆还细分为：新生代和老年代，其中新生代又可以分为Eden空间、From Survivor空间、To Survivor空间，HotSpot虚拟机默认Eden和Survivor的大小比例是8:1。根据Java虚拟机规范规定，Java堆可以处在物理上不连续的内存空间中，只要逻辑连续就行了。目前主流的虚拟机都可以通过命令来自主设置Java堆的大小，其中命令-Xms1024m指的是Java堆的初始大小，-Xmx2048m指的是分配给Java堆的最大内存。 方法区方法区和Java堆一样也是所有线程共享的内存区域，它用于存储已被虚拟机加载的类信息、常量、静态变量、即时编译器编译后的代码等数据。有些在HotSpot虚拟机上开发、部署程序的开发者习惯称方法区为“永久代（Permanent Generation）”，这是由于HotSpot虚拟机的垃圾收集器可以像管理Java堆一样管理方法区这块内存区域，省去了专门为方法区编写内存管理代码的工作。 运行时常量池运行时常量池（Runtime Constant Pool）是方法区的一部分。Class文件中除了有类的版本、字段、方法、接口等描述信息外，还有一项信息是常量池，用于存放编译期生成的各种字面量和符号引用，这部分内容将在类加载后进入方法区的运行时常量池。运行时常量池相对于Class文件常量池的另一个重要特征是具备动态性，Java语言并不要求常量一定只有编译期才能产生，也就是并非预置入Class文件中常量池的内容才能进入方法区运行时常量池，运行期间也可能将新的常量放入池中，比如String类中的intern()方法。 直接内存直接内存（Direct Memory）并不是虚拟机运行时数据区的一部分，也不是Java虚拟机规范中定义的内存区域，但是这部分内存也被频繁使用。自从JDK1.4中引入NIO（New Input/Output）类，提出一种基于通道（Channel）和缓冲区（Buffer）的I/O方式，它可以使用Native方法直接分配堆外内存，然后通过一个存储在Java堆中的DirectByteBuffer对象作为这块内存的引用进行操作。 对象创建过程虚拟机遇到一条new指令，首先检查这个指令的参数是否能在常量池中定位到一个类的符号引用，并且检查这个符号引用代表的类是否已被加载、解析和初始化过。如果没有就必须先执行相关的类加载过程。在类加载检查通过之后，接下来虚拟机将为新生对象分配内存。对象所需内存的大小在类加载完成后就可以完全确定，为对象分配空间的任务就是把一块确定大小的内存从Java堆中划分出来。假设Java堆中内存是绝对规整的，所有用过的内存都放在一边，空闲的内存放在另一边，中间放着一个指针作为分界点的指示器，那么分配内存就是把指针往空闲内存的方向挪动一段与对象大小相等的距离，这种分配方式称为“指针碰撞（Bump the Pointer）”；如果Java堆中的内存不规整，那么虚拟机就必须维护一个列表，记录哪些内存块可用，在为对象分配内存的时候就从列表中寻找一块足够大的空间划分给对象实例，并更新表上的记录，这种分配方式称为“空闲列表（Free List）”。]]></content>
      <categories>
        <category>深入理解Java虚拟机</category>
      </categories>
      <tags>
        <tag>Java堆</tag>
        <tag>方法区</tag>
        <tag>Java虚拟机栈</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[cuda安装]]></title>
    <url>%2F2019%2F01%2F08%2Fcuda%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[cuda安装cuda和cudnn版本查看 cuda版本 1cat /usr/local/cuda/version.txt cudnn版本 1cat /usr/local/cuda/include/cudnn.h | grep CUDNN_MAJOR -A 2 cuda8.0卸载 卸载 1apt autoremove cuda 清除残留文件 12cd /usr/local/rm -rf cuda-8.0/ cuda9.0及对应cudnn安装cuda9.0安装cuda9.0官方网站 运行压缩.run 1sudo sh cuda_9.0.176_384.81_linux.run 一般在不需要图形驱动（Grphics Driver）和 样例（cuda samples） cudnn官方网站 2.~/.bashrc配置 123export CUDA_HOME=/usr/local/cudaexport LD_LIBRARY_PATH=$&#123;CUDA_HOME&#125;/lib64export PATH=$&#123;CUDA_HOME&#125;/bin:$&#123;PATH&#125; 刷新使得配置生效 1source ~/.bashrc cudnn安装本次版本是cuDNN v7.4.2 (Dec 14, 2018), for CUDA 9.0，选择cuDNN Library for Linux。 解压cudnn-9.0-linux-x64-v7.4.2.24.tgz压缩包 1tar -zxvf cudnn-9.0-linux-x64-v7.4.2.24.tgz 复制文件到cuda库下 12cp cuda/lib64/* /usr/local/cuda-9.0/lib64/cp cuda/include/* /usr/local/cuda-9.0/include/ 查看cudnn版本信息 1cat /usr/local/cuda/include/cudnn.h | grep CUDNN_MAJOR -A 2 cuda切换当一台服务器上既安装了cuda8.0和cuda9.0，使用ls命令查看/usr/local下的文件包含三个文件夹cuda、cuda8.0和cuda9.0。 1ls -l /usr/local/ 可以看到，当前cuda文件夹链接到cuda-9.0,当需要切换到cuda8.0时，使用以下命令： 123rm -rf /usr/local/cuda #删除之前创建的软链接sudo ln -s /usr/local/cuda-8.0/ /usr/local/cudanvcc --version #查看当前 cuda 版本 参考博客：http://geyao1995.com/CUDA8_CUDA9/]]></content>
      <categories>
        <category>科研</category>
      </categories>
      <tags>
        <tag>环境配置</tag>
        <tag>软件安装</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hierarchical Object Detection with Deep Reinforcement Learning]]></title>
    <url>%2F2019%2F01%2F08%2FHierarchical-Object-Detection-with-Deep-Reinforcement-Learning%2F</url>
    <content type="text"><![CDATA[Hierarchical Object Detection with Deep Reinforcement LearningAbstract 关键的思想在于关注图像中包含更丰富信息的那些部分并放大它们。 Introduction 考虑区域之间的联系， 利用top-down的扫描方式，首先获取整个图像，关注局部区域的相关信息， 基于增强学习训练的代理（agent）有能力检测图像中的对象 Hierarchical Object Detection Model1. Markov Decision Process（马尔科夫决策过程） State:当前区域描述符（the descriptor of current region）和记忆向量（memory vector） Actions:move actions和terminal actions Reward:保证move action都是朝着更靠近ground truth的方向移动；当IOU超过threshold，则终止移动。 2. Q-learning1Q(s,a) = r+\lambda&#123;max&#125;_&#123;a&apos;&#125;Q(s&apos;,a&apos;) 3. Model the Image-Zooms model 使用VGG-16提取图像区域特征向量$(7*7*512)$,拼接区域特征向量与记忆向量（memory vector）$(7*7*512+24=25088+24)$,经过两个1024维的全连接层，输出6个可能的动作（actions），反复迭代，直到终止动作 the Pool45-Crops model 4. TrainingExperiments1. Qualitative Results Implementation1. keras实现 提取区域特征 state不断更新，并作为model的输入 1(7*7*512) 代码问题总结 数据类型错误TypeError: slice indices must be integers or None or have an index method 这是由于数组，矩阵等类型数据的下标是整数，而在 12region_mask[offset[0]:offset[0] + size_mask[0] , offset[1]:offset[1] + size_mask[1]] = 1 offset是float类型，所以报错，解决方法就是数据类型转换： 12region_mask[int(offset[0]):int(offset[0] + size_mask[0]) , int(offset[1]):int(offset[1] + size_mask[1])] = 1 VGG16提取图像特征尺寸不对 解决方法：在图片提取特征之前，对图像进行resize； 1im = images[z].resize((224, 224)) 除0错误 对图像进行resize的位置错误 问题总结 问题：记忆向量的哪儿来的？ 问题：哪6个动作？ move actions:左上、右上、左下、右下和中；terminal actions 问题：每个类训练一个模型？ 是的，这篇文章中只训练了飞机类（aeroplane）的检测模型]]></content>
      <categories>
        <category>科研</category>
      </categories>
      <tags>
        <tag>对象检测</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Yotta系统错误记录及性能优化]]></title>
    <url>%2F2019%2F01%2F07%2FYotta%E7%B3%BB%E7%BB%9F%E9%94%99%E8%AF%AF%E8%AE%B0%E5%BD%95%E5%8F%8A%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%2F</url>
    <content type="text"><![CDATA[错误记录 在日志系统运行期间，每隔几天kafka平台就会自动崩溃，控制台窗口关闭，并且重新启动kafka平台也会出现日志文件冲突问题，从而导致日志系统无法正常记录用户行为日志。 解决方法： 首先，我们查阅了官网的各种文档，在全面了解kafka的实现原理与应用之后仍然没有找到解决问题的方法。然后我们在StackOverflow和Quora上进行了相关问题的查看，尝试后发现类似问题解决方法，将kafka平台对应的日志文件删除就可以正常启动kafka平台。虽然通过删除日志文件的方式使得日志记录正常进行，但是依旧没有解决每隔几天kafka平台就会自动崩溃的问题，频繁人工启动不稳定的服务耗费人力开销。 为了彻底解决这个问题，我们维护人员仔细阅读kafka生成的日志文件，了解到崩溃的发生是因为日志文件大小达到阈值时kafka程序会对日志文件重命名，而此时日志文件又被自身所占用，就产生了程序异常，在Apache官网issues下发现这是此版本kafka在windows下的一个错误，所以我们将日志系统整体迁移到linux系统下，将问题完美解决。 性能优化 Optimizing MySQL LIKE ‘%string%’ queries 方法1：建立fulltext索引，全文索引只能用于数据库引擎为MYISAM的数据表，但是全文索引不支持中文； 方法2：]]></content>
      <categories>
        <category>工程</category>
      </categories>
      <tags>
        <tag>yotta系统</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[图像文本匹配相关工作]]></title>
    <url>%2F2018%2F11%2F07%2F%E5%9B%BE%E5%83%8F%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[图像文本匹配相关工作introduction 什么是图像文本匹配？计算机视觉任务逐渐不在满足于简单的图像分类、或者为图像分配一个或几个标签的任务，越来越多的研究者希望能够通过匹配图像和文本，为图像生成丰富的文本描述，从而更好地理解图像的语义。 已经有大量的研究工作，这些工作的方法怎么做的？ 我的工作在别人的工作上有什么改进？有什么优点、贡献？ 通过生成网络生成更多的正例(positive fact), 从而扩充训练数据集; 设计了一个高效的训练算法，交替优化生成网络和判别网络的参数，得到强判别器； 在多个数据集上实现了与其他方法可比较甚至更优异的结果。 related work1. CCA Canonical Correlation Analysis (CCA) Kernel Canonical Correlation Analysis (KCCA) deep CCA Sparse Kernel CCA Randomized CCA Nonparametric CCA 2. ranking based method Y. Verma and C. Jawahar, “Im2text and text2im: Associating images and texts for cross-modal retrieval,” in British Machine Vision Conference (BMVC), vol. 1, 2014, p. 2. R. Socher, A. Karpathy, Q. V. Le, C. D. Manning, and A. Y. Ng, “Grounded compositional semantics for finding and describing images with sentences,” Transactions of the Association for Computational Linguistics, vol. 2, pp. 207–218, 2014. OK A. Karpathy, A. Joulin, and F. F. F. Li, “Deep fragment embeddings for bidirectional image sentence mapping,” in Neural Information Processing Systems (NIPS), 2014, pp. 1889–1897. OK R. Kiros, R. Salakhutdinov, and R. S. Zemel, “Unifying visual-semantic embeddings with multimodal neural language models,” arXiv preprint arXiv:1411.2539, 2014. OK L. Wang, Y. Li, and S. Lazebnik, “Learning deep structure-preserving image-text embeddings,” in Computer Vision and Pattern Recognition (CVPR), 2016, pp. 5005–5013. “Learning two-branch neural networks for image-text matching tasks,” arXiv preprint arXiv:1704.03470, 2017. Huang Y, Wang W, Wang L. Instance-aware image and sentence matching with selective multimodal lstm[C]//The IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 2017, 2(6): 7. Huang Y, Wu Q, Wang L. Learning semantic concepts and order for image and sentence matching[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018: 6163-6171. 3.Generative Adversarial Networks(GANs) @inproceedings{goodfellow2014generative,title={Generative adversarial nets},author={Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},booktitle={Advances in neural information processing systems},pages={2672–2680},year={2014}} @article{mirza2014conditional,title={Conditional generative adversarial nets},author={Mirza, Mehdi and Osindero, Simon},journal={arXiv preprint arXiv:1411.1784},year={2014}} @article{radford2015unsupervised,title={Unsupervised representation learning with deep convolutional generative adversarial networks},author={Radford, Alec and Metz, Luke and Chintala, Soumith},journal={arXiv preprint arXiv:1511.06434},year={2015}} @article{reed2016generative,title={Generative adversarial text to image synthesis},author={Reed, Scott and Akata, Zeynep and Yan, Xinchen and Logeswaran, Lajanugen and Schiele, Bernt and Lee, Honglak},journal={arXiv preprint arXiv:1605.05396},year={2016}} @inproceedings{wang2017irgan,title={Irgan: A minimax game for unifying generative and discriminative information retrieval models},author={Wang, Jun and Yu, Lantao and Zhang, Weinan and Gong, Yu and Xu, Yinghui and Wang, Benyou and Zhang, Peng and Zhang, Dell},booktitle={Proceedings of the 40th International ACM SIGIR conference on Research and Development in Information Retrieval},pages={515–524},year={2017},organization={ACM}} @article{cai2017kbgan,title={Kbgan: Adversarial learning for knowledge graph embeddings},author={Cai, Liwei and Wang, William Yang},journal={arXiv preprint arXiv:1711.04071},year={2017}} experiment 数据集的扩充：对每一张图片进行裁剪，4个角以及中间，并将这5个裁剪的图翻转，一张图片扩充得到10张尺寸为$128 \times 128$的图片。]]></content>
      <categories>
        <category>科研</category>
      </categories>
      <tags>
        <tag>图文匹配</tag>
        <tag>典型相关分析</tag>
        <tag>排序损失</tag>
        <tag>数据集</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[研究生数学建模：恐怖袭击事件分级]]></title>
    <url>%2F2018%2F09%2F15%2F%E7%A0%94%E7%A9%B6%E7%94%9F%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%EF%BC%9A%E6%81%90%E6%80%96%E8%A2%AD%E5%87%BB%E4%BA%8B%E4%BB%B6%E5%88%86%E7%BA%A7%2F</url>
    <content type="text"><![CDATA[研究生数学建模：恐怖袭击事件分级特征：人员伤亡、经济损失、时间、地点（人口容量、GDP）等 财产损失特征：攻击类型、武器类型、受害子类型、国家、地区、入选标准（1、2、3、doubtterr） 攻击类型 暗杀（1/0） 武装袭击（1/0） 轰炸爆炸（1/0） 劫持（1/0） 设施攻击（1/0） 徒手攻击（1/0） 未知（1/0） 攻击成功（1/0） 自杀式袭击（suicide）武器类型 生化武器、放射性武器（1/0） 核武器 轻武器 炸弹 燃烧武器 治乱武器 交通工具 破坏设备 未知 受害者类型 商业 政府 警察 军事 流产有关 运输（机场（飞机）或巴士、火车、高铁运输） 教育机构 食物或水供应 媒体设施 海事 非政府组织 其他 地区 北美 南美 东亚 东南亚 南亚 中亚 西欧 东欧 中东和北非 撒哈拉以南非洲 澳大利亚 入选标准 标准1 标准2 标准3 – 疑似恐怖主义 输出： 灾难性的 重大的 较小的 无损失 未知 设置propextent中为空的部分为0，这样就可以去除property K-means聚类结果分析 样本数110953\times 17 聚类结果（5类） 统计 类1 类2 类3 类4 类5 数量 2497 4645 134 1854 1823 Spectral Clustering聚类结果分析 样本数110953\times 17 聚类结果（5类） 统计 类1 类2 类3 类4 类5 数量 10883 1 15 1 53 分段后K-means聚类结果分析 样本数110953\times 36 聚类结果（5类） 统计 类1 类2 类3 类4 类5 数量 6339 1541 1474 725 874 分段后Spectral Clustering聚类结果分析 样本数110953\times 36 聚类结果（5类） 统计 类1 类2 类3 类4 类5 数量 9472 26 1343 93 19 增大死亡数以及财产损失权重，分段后K-means聚类结果分析 样本数110953\times 36 聚类结果（5类） 统计 类1 类2 类3 类4 类5 数量 1541 735 7203 1474 0 增大死亡数以及财产损失权重，分段后Spectral Clustering聚类结果分析 样本数110953\times 36 聚类结果（5类） 统计 类1 类2 类3 类4 类5 数量 9471 1 1349 39 93]]></content>
      <categories>
        <category>竞赛</category>
      </categories>
      <tags>
        <tag>编程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[智慧教育师范应用表格接口规范]]></title>
    <url>%2F2018%2F07%2F10%2F%E6%99%BA%E6%85%A7%E6%95%99%E8%82%B2%E5%B8%88%E8%8C%83%E5%BA%94%E7%94%A8%E8%A1%A8%E6%A0%BC%E6%8E%A5%E5%8F%A3%E8%A7%84%E8%8C%83%2F</url>
    <content type="text"><![CDATA[智慧教育-网络学院示范应用后端数据访问接口主题状态1. 主题状态表 列名 类型 长度 state_id bigint(long) 20 domain_id bigint(long) 20 states varchar(string) 255 user_id bigint(long) 20 created_time datetime 1 modified_time datetime 1 说明： （1） 学习状态：0表示未学习，1表示正在学习，2表示已学习； （2） states（主题状态列表）形式：学习状态1，学习状态2，学习状态3，…… 举例： 1， 0， 1， 2，…… 2. 主题状态接口 （1） /topicState/getByDomainIdAndUserId 查询主题状态，参数 long domainId long userId 课程id 用户id （2） /topicState/saveStateByDomainIdAndUserId 保存主题状态，参数 long domainId String states long userId 课程id 主题状态列表 用户id （3） /topicState/saveStateByDomainNameAndUserId 保存主题状态，参数 long domainName String states long userId 课程名 主题状态列表 用户id 分面状态1. 分面状态表 列名 类型 长度 state_id bigint(long) 20 domain_id bigint(long) 20 topic_id bigint(long) 20 states varchar(string) 255 user_id bigint(long) 20 created_time datetime 1 modified_time datetime 1 说明： （1） 学习状态：0表示未学习，1表示已在学习； （2） states（分面状态列表）形式：学习状态1，学习状态2，学习状态3，…… 举例： 1， 0， 1， 0，…… 2. 分面状态接口 （1）/facetState/getByDomainIdAndTopicIdAndUserId 查询分面状态，参数 long domainId long userId long topicId 课程id 用户id 主题id （2）/facetState/saveStateByDomainIdAndTopicIdAndUserId 保存分面状态，参数 long domainId long topicId String states long userId 课程id 主题id 分面状态列表 用户id （3）/facetState/saveStateByDomainNameAndTopicNameAndUserId 保存分面状态，参数 string domainName string topicName String states long userId 课程名 主题名 分面状态列表 用户id （4）/facetState/saveStateByDomainIdAndUserId 保存分面状态，参数 long domainId String states long userId 课程id 分面状态列表 用户id （5）/facetState/saveStateByDomainNameAndUserId 保存主题状态，参数 long domainName String states long userId 课程名 分面状态列表 用户id 注：states：分面状态的矩阵（行（主题）之间以分号隔开，行内以逗号隔开） 例如：0,0,1,0;0,0,1;1,1,0;1,0,1,1,1;……推荐主题1. 推荐主题表 列名 类型 长度 recommendation_id bigint(long) 20 domain_id bigint(long) 20 recommendation_topics varchar(string) 255 user_id bigint(long) 20 created_time datetime 1 modified_time datetime 1 说明： （1）recommendation_topics（推荐主题列表）形式：：推荐主题1 id，推荐主题2 id，推荐主题3 id；推荐主题3 id，推荐主题1 id，推荐主题4 id；……即，不同推荐方式之间以分号隔开，同一推荐方式内的主题id以逗号分隔开 2. 2. 推荐主题接口 （1） recommendation/getByDomainIdAndUserId 查询推荐主题，参数 long domainId long userId 课程id 用户id （2） recommendation/saveRecommendationByDomainIdAndUserId 保存推荐主题，参数 long domainId String recommendationTopics long userId 课程id 推荐主题列表 用户id （3） recommendation/saveRecommendationByDomainNameAndUserId 保存推荐主题，参数 long domainName String recommendationTopics long userId 课程名 推荐主题列表 用户id]]></content>
      <categories>
        <category>工程</category>
      </categories>
      <tags>
        <tag>数据库</tag>
        <tag>智慧教育系统</tag>
      </tags>
  </entry>
</search>
